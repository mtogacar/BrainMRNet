{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"cells":[{"cell_type":"code","metadata":{"id":"oY4ffYPhGdIE","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1580647452352,"user_tz":-180,"elapsed":50913,"user":{"displayName":"mesut toğaçar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZ38OP6Ot7SYBDlVMe9pPMniBHAPfeMnUf9s91=s64","userId":"08628900079395750792"}},"outputId":"bd8fa00b-d827-46b7-81dc-ad37da5eff2f"},"source":["import os\n","import warnings\n","warnings.filterwarnings('ignore')\n","import sys\n","sys.path.insert(0, '/colab/brainmrnet/')\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from PIL import Image\n","from albumentations import *\n","from skimage.transform import resize\n","\n","\n","from sklearn.model_selection import KFold, train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score\n","\n","from keras.layers import *\n","from keras.callbacks import *\n","from keras.optimizers import *\n","from keras.models import load_model, Model\n","\n","import tensorflow as tf\n","from keras import backend as K\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","!ls \"/content/drive/My Drive/colab/brainmrnet/data/\"\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sl3WnfTa4jYm"},"source":["SHAPE = (224, 224, 3)\n","BATCH_SIZE = 64\n","EPOCHS = 100\n","N_SPLITS = 5\n","SEED = 1881\n","TRAIN_TEST_RATIO = 0.3\n","\n","BASE_DIR     = ('/content/drive/My Drive/colab/brainmrnet/data/data/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uGT_8VyTZt2T"},"source":["class DATASET:\n","\n","    \"\"\"\n","    input_shape           --> TUPLE.wanted image size\n","    batch_size            --> INT.yielding data size for every iteration\n","    orders                --> LIST.which images will be used. max=len(all_images). it can be used for K-fold(CV).\n","    base_dir              --> STR.the DIR which is include images.\n","    seed                  --> INT. This allow to dataset generator to more reproduciable and it ensures that x and y are shuffled with compatible.\n","    augment               --> BOOL. Augment data or not.\n","    train_test_ratio      --> How much of data will be used as test set.\n","    \"\"\"\n","    \n","    def __init__(self, input_shape, batch_size, orders, base_dir, seed, train_test_ratio, augment=True):\n","        self.SHAPE                 = input_shape\n","        self.BATCH_SIZE            = batch_size\n","        self.arr                   = orders\n","        self.SEED                  = seed\n","        self.TT_RATIO              = train_test_ratio\n","        self.AUG                   = augment\n","        \n","        self.BASE_DIR              = base_dir\n","        \n","        \n","    def get_paths_n_labels(self):\n","\n","        x      = []\n","        label = []\n","\n","        img_paths = os.listdir(self.BASE_DIR)\n","        \n","        for img_path in img_paths:\n","           \n","            if (\"men\" in img_path) or (\"MEN\" in img_path):\n","                x.append(os.path.join(self.BASE_DIR,img_path))\n","                label.append([1,0,0])\n","            elif \"gli\" in img_path:\n","                x.append(os.path.join(self.BASE_DIR,img_path))\n","                label.append([0,1,0])\n","            elif \"pit\" in img_path:\n","                x.append(os.path.join(self.BASE_DIR,img_path))\n","                label.append([0,0,1])\n","                \n","        return x, label\n","    \n","    def __len__(self):\n","        return len(self.get_paths_n_labels()[0])\n","    \n","    def get_img(self, img_path):\n","        img = Image.open(img_path)\n","        return np.array(img)\n","    \n","    def augmenting(self, img):\n","        if self.AUG:\n","            augment = Compose([VerticalFlip(p=0.2),\n","                               HorizontalFlip(p=0.2),\n","                               RandomBrightnessContrast(p=0.2),\n","                               ShiftScaleRotate(p=0.2, shift_limit=0.0, scale_limit=0.05, rotate_limit=20)])   \n","        else:\n","            augment = Compose([])  \n","\n","        img = augment(image=img)['image']\n","        return img\n","    \n","    \n","    def resize_and_normalize(self, img):\n","        img = resize(img, self.SHAPE)\n","        return img\n","    \n","    def get_shuffled_data(self):\n","        img_paths, labels = self.get_paths_n_labels()\n","\n","        np.random.seed(self.SEED) \n","        np.random.shuffle(img_paths)\n","        \n","        np.random.seed(self.SEED) \n","        np.random.shuffle(labels)\n","        \n","        return img_paths, labels\n","        \n","    def split_train_test(self, get):  # get=={\"train\",\"test\"}\n","        img_paths, labels = self.get_shuffled_data()\n","        x_train, x_test, y_train, y_test = train_test_split(img_paths, labels, test_size=self.TT_RATIO, random_state=self.SEED)\n","        \n","        if get=='train':\n","            return x_train, y_train\n","        \n","        elif get=='test':\n","            return x_test, y_test\n","    \n","    def data_generator(self):\n","        img_paths, labels = self.split_train_test(get=\"train\")\n","        \n","        while True:\n","            x = np.empty((self.BATCH_SIZE,)+self.SHAPE, dtype=np.float32)\n","            y = np.empty((self.BATCH_SIZE, 3), dtype=np.float32)\n","\n","            batch = np.random.choice(self.arr, self.BATCH_SIZE)\n","\n","            for ix, id_ in enumerate(batch):\n","                # x\n","                img_path = img_paths[id_]\n","                img = self.get_img(img_path)\n","                img = self.augmenting(img)\n","                img = self.resize_and_normalize(img)\n","                  \n","                # y \n","                label = labels[id_]\n","             \n","                # Store the values    \n","                x[ix] = img\n","                y[ix] = label\n","\n","            yield x, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bJ-aVohsqt1F","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1580647469315,"user_tz":-180,"elapsed":2430,"user":{"displayName":"mesut toğaçar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZ38OP6Ot7SYBDlVMe9pPMniBHAPfeMnUf9s91=s64","userId":"08628900079395750792"}},"outputId":"9ffdafac-fbe2-47b0-bad2-3b42f623a61b"},"source":["dataset = DATASET(SHAPE, 1, range(4), BASE_DIR, SEED, TRAIN_TEST_RATIO, augment=True)\n","\n","for ix, data in enumerate(dataset.data_generator()):\n","    img, y = data\n","    print(img)\n","    print(img.shape)\n","    print(\"-\"*10)\n","    print(y)\n","    print(y.shape)\n","    print(\"-\"*10)\n","    print(img[0,:,:,:].shape)\n","    plt.imshow(img[0,:,:,:])\n","    plt.show()\n","    \n","    if ix==0:\n","        break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[[[0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]\n","   ...\n","   [0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]]\n","\n","  [[0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]\n","   ...\n","   [0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]]\n","\n","  [[0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]\n","   ...\n","   [0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]]\n","\n","  ...\n","\n","  [[0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]\n","   ...\n","   [0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]]\n","\n","  [[0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]\n","   ...\n","   [0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]]\n","\n","  [[0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]\n","   ...\n","   [0. 0. 0.]\n","   [0. 0. 0.]\n","   [0. 0. 0.]]]]\n","(1, 224, 224, 3)\n","----------\n","[[0. 0. 1.]]\n","(1, 3)\n","----------\n","(224, 224, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"NZygcwwp0Sry"},"source":["# credits: https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n","def recall(y_true, y_pred):\n","    \"\"\"\n","    Recall metric.\n","    \n","    Only computes a batch-wise average of recall.\n","    \n","    Computes the recall, a metric for multi-label classification of\n","    how many relevant items are selected.\n","    \"\"\"\n","    \n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","def precision(y_true, y_pred):\n","    \"\"\"Precision metric.\n","    \n","    Only computes a batch-wise average of precision.\n","    \n","    Computes the precision, a metric for multi-label classification of\n","    how many selected items are relevant.\n","    \"\"\"\n","    \n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","def f1(y_true, y_pred):\n","    precisionx = precision(y_true, y_pred)\n","    recallx = recall(y_true, y_pred)\n","    return 2*((precisionx*recallx)/(precisionx+recallx+K.epsilon()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bBbVuHAfgHnh"},"source":["class SGDRScheduler(Callback):\n","    '''Cosine annealing learning rate scheduler with periodic restarts.\n","    # Usage\n","        ```python\n","            schedule = SGDRScheduler(min_lr=1e-5,\n","                                     max_lr=1e-2,\n","                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n","                                     lr_decay=0.9,\n","                                     cycle_length=5,\n","                                     mult_factor=1.5)\n","            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n","        ```\n","    # Arguments\n","        min_lr: The lower bound of the learning rate range for the experiment.\n","        max_lr: The upper bound of the learning rate range for the experiment.\n","        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n","        lr_decay: Reduce the max_lr after the completion of each cycle.\n","                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n","        cycle_length: Initial number of epochs in a cycle.\n","        mult_factor: Scale epochs_to_restart after each full cycle completion.\n","    # References\n","        Blog post: jeremyjordan.me/nn-learning-rate\n","        Original paper: http://arxiv.org/abs/1608.03983\n","    '''\n","    def __init__(self,\n","                 min_lr,\n","                 max_lr,\n","                 steps_per_epoch,\n","                 lr_decay=0.9, #1 oldu\n","                 cycle_length=5,\n","                 mult_factor=1.5):  #2 oldu\n","\n","        self.min_lr = min_lr\n","        self.max_lr = max_lr\n","        self.lr_decay = lr_decay\n","\n","        self.batch_since_restart = 0\n","        self.next_restart = cycle_length\n","\n","        self.steps_per_epoch = steps_per_epoch\n","\n","        self.cycle_length = cycle_length\n","        self.mult_factor = mult_factor\n","\n","        self.history = {}\n","\n","    def clr(self):\n","        '''Calculate the learning rate.'''\n","        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n","        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n","        return lr\n","\n","    def on_train_begin(self, logs={}):\n","        '''Initialize the learning rate to the minimum value at the start of training.'''\n","        logs = logs or {}\n","        K.set_value(self.model.optimizer.lr, self.max_lr)\n","\n","    def on_batch_end(self, batch, logs={}):\n","        '''Record previous batch statistics and update the learning rate.'''\n","        logs = logs or {}\n","        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n","        for k, v in logs.items():\n","            self.history.setdefault(k, []).append(v)\n","\n","        self.batch_since_restart += 1\n","        K.set_value(self.model.optimizer.lr, self.clr())\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        '''Check for end of current cycle, apply restarts when necessary.'''\n","        if epoch + 1 == self.next_restart:\n","            self.batch_since_restart = 0\n","            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n","            self.next_restart += self.cycle_length\n","            self.max_lr *= self.lr_decay\n","            self.best_weights = self.model.get_weights()\n","\n","    def on_train_end(self, logs={}):\n","        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n","        self.model.set_weights(self.best_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvvEuAAKfkV5"},"source":["# copied from https://github.com/kobiso/CBAM-keras/blob/master/models/attention_module.py\n","def cbam_block(cbam_feature, ratio=8):\n","    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n","    As described in https://arxiv.org/abs/1807.06521.\n","    \"\"\"\n","    \n","    cbam_feature = channel_attention(cbam_feature, ratio)\n","    cbam_feature = spatial_attention(cbam_feature)\n","    return cbam_feature\n","\n","def channel_attention(input_feature, ratio=8):\n","    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n","    channel = input_feature._keras_shape[channel_axis]\n","    \n","    shared_layer_one = Dense(channel//ratio,\n","                             activation='relu',\n","                             kernel_initializer='he_normal',\n","                             use_bias=True,\n","                             bias_initializer='zeros')\n","    shared_layer_two = Dense(channel,\n","                             kernel_initializer='he_normal',\n","                             use_bias=True,\n","                             bias_initializer='zeros')\n","    \n","    avg_pool = GlobalAveragePooling2D()(input_feature)    \n","    avg_pool = Reshape((1,1,channel))(avg_pool)\n","    assert avg_pool._keras_shape[1:] == (1,1,channel)\n","    avg_pool = shared_layer_one(avg_pool)\n","    assert avg_pool._keras_shape[1:] == (1,1,channel//ratio)\n","    avg_pool = shared_layer_two(avg_pool)\n","    assert avg_pool._keras_shape[1:] == (1,1,channel)\n","    \n","    max_pool = GlobalMaxPooling2D()(input_feature)\n","    max_pool = Reshape((1,1,channel))(max_pool)\n","    assert max_pool._keras_shape[1:] == (1,1,channel)\n","    max_pool = shared_layer_one(max_pool)\n","    assert max_pool._keras_shape[1:] == (1,1,channel//ratio)\n","    max_pool = shared_layer_two(max_pool)\n","    assert max_pool._keras_shape[1:] == (1,1,channel)\n","    \n","    cbam_feature = Add()([avg_pool,max_pool])\n","    cbam_feature = Activation('sigmoid')(cbam_feature)\n","\n","    if K.image_data_format() == \"channels_first\":\n","        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n","    \n","    return multiply([input_feature, cbam_feature])\n","\n","def spatial_attention(input_feature):\n","    kernel_size = 7\n","    \n","    if K.image_data_format() == \"channels_first\":\n","        channel = input_feature._keras_shape[1]\n","        cbam_feature = Permute((2,3,1))(input_feature)\n","    else:\n","        channel = input_feature._keras_shape[-1]\n","        cbam_feature = input_feature\n","    \n","    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n","    assert avg_pool._keras_shape[-1] == 1\n","    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n","    assert max_pool._keras_shape[-1] == 1\n","    concat = Concatenate(axis=3)([avg_pool, max_pool])\n","    assert concat._keras_shape[-1] == 2\n","    cbam_feature = Conv2D(filters = 1,\n","                    kernel_size=kernel_size,\n","                    strides=1,\n","                    padding='same',\n","                    activation='sigmoid',\n","                    kernel_initializer='he_normal',\n","                    use_bias=False)(concat)\t\n","    assert cbam_feature._keras_shape[-1] == 1\n","    \n","    if K.image_data_format() == \"channels_first\":\n","        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n","        \n","    return multiply([input_feature, cbam_feature])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4Ndx2vm6NZ3"},"source":["# copied from https://gist.github.com/mjdietzx/5319e42637ed7ef095d430cb5c5e8c64\n","def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n","    shortcut = y\n","\n","    # down-sampling is performed with a stride of 2\n","    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n","    y = BatchNormalization()(y)\n","    y = LeakyReLU()(y)\n","\n","    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n","    y = BatchNormalization()(y)\n","\n","    # identity shortcuts used directly when the input and output are of the same dimensions\n","    if _project_shortcut or _strides != (1, 1):\n","        # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n","        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n","        shortcut = Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n","        shortcut = BatchNormalization()(shortcut)\n","\n","    y = add([shortcut, y])\n","    y = LeakyReLU()(y)\n","\n","    return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVUWz9lzfm6Y"},"source":["def create_model():\n","    \n","    dropRate = 0.3\n","    \n","    init = Input(SHAPE)\n","    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x1 = MaxPooling2D((2,2))(x)\n","    \n","    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = cbam_block(x)\n","    x = residual_block(x, 64)\n","    x2 = MaxPooling2D((2,2))(x)\n","    \n","    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = cbam_block(x)\n","    x = residual_block(x, 128)\n","    x3 = MaxPooling2D((2,2))(x)\n","    \n","    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1)\n","    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2)\n","    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3)\n","    \n","    hypercolumn = Concatenate()([ginp1, ginp2, ginp3]) \n","    gap = GlobalAveragePooling2D()(hypercolumn)\n","\n","    x = Dense(256, activation=None)(gap)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = Dropout(dropRate)(x)\n","    \n","    x = Dense(256, activation=None)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    y = Dense(3, activation='softmax')(x)\n","   \n","    model = Model(init, y)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2V3AUW7fm-n","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1580647513137,"user_tz":-180,"elapsed":9503,"user":{"displayName":"mesut toğaçar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZ38OP6Ot7SYBDlVMe9pPMniBHAPfeMnUf9s91=s64","userId":"08628900079395750792"}},"outputId":"8bc2d8c1-1fbf-41db-942d-60f17a0848d1"},"source":["model = create_model()\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2241: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 224, 224, 32) 896         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 224, 224, 32) 128         conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 224, 224, 32) 0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 224, 224, 32) 9248        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 224, 224, 32) 128         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 224, 224, 32) 0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 112, 112, 32) 0           activation_2[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 112, 112, 64) 18496       max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 112, 112, 64) 256         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 112, 112, 64) 0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_1 (Glo (None, 64)           0           activation_3[0][0]               \n","__________________________________________________________________________________________________\n","global_max_pooling2d_1 (GlobalM (None, 64)           0           activation_3[0][0]               \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 1, 1, 64)     0           global_average_pooling2d_1[0][0] \n","__________________________________________________________________________________________________\n","reshape_2 (Reshape)             (None, 1, 1, 64)     0           global_max_pooling2d_1[0][0]     \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 1, 1, 8)      520         reshape_1[0][0]                  \n","                                                                 reshape_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 1, 1, 64)     576         dense_1[0][0]                    \n","                                                                 dense_1[1][0]                    \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 1, 1, 64)     0           dense_2[0][0]                    \n","                                                                 dense_2[1][0]                    \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 1, 1, 64)     0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","multiply_1 (Multiply)           (None, 112, 112, 64) 0           activation_3[0][0]               \n","                                                                 activation_4[0][0]               \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 112, 112, 1)  0           multiply_1[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 112, 112, 1)  0           multiply_1[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 112, 112, 2)  0           lambda_1[0][0]                   \n","                                                                 lambda_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 112, 112, 1)  98          concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","multiply_2 (Multiply)           (None, 112, 112, 64) 0           multiply_1[0][0]                 \n","                                                                 conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 112, 112, 64) 36928       multiply_2[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 112, 112, 64) 256         conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)       (None, 112, 112, 64) 0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 112, 112, 64) 36928       leaky_re_lu_1[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 112, 112, 64) 256         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 112, 112, 64) 0           multiply_2[0][0]                 \n","                                                                 batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)       (None, 112, 112, 64) 0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 56, 56, 64)   0           leaky_re_lu_2[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 56, 56, 128)  73856       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 56, 56, 128)  512         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 56, 56, 128)  0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_2 (Glo (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","global_max_pooling2d_2 (GlobalM (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","reshape_3 (Reshape)             (None, 1, 1, 128)    0           global_average_pooling2d_2[0][0] \n","__________________________________________________________________________________________________\n","reshape_4 (Reshape)             (None, 1, 1, 128)    0           global_max_pooling2d_2[0][0]     \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 1, 1, 16)     2064        reshape_3[0][0]                  \n","                                                                 reshape_4[0][0]                  \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1, 1, 128)    2176        dense_3[0][0]                    \n","                                                                 dense_3[1][0]                    \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 1, 1, 128)    0           dense_4[0][0]                    \n","                                                                 dense_4[1][0]                    \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 1, 1, 128)    0           add_3[0][0]                      \n","__________________________________________________________________________________________________\n","multiply_3 (Multiply)           (None, 56, 56, 128)  0           activation_5[0][0]               \n","                                                                 activation_6[0][0]               \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 56, 56, 1)    0           multiply_3[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 56, 56, 1)    0           multiply_3[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 56, 56, 2)    0           lambda_3[0][0]                   \n","                                                                 lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 56, 56, 1)    98          concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","multiply_4 (Multiply)           (None, 56, 56, 128)  0           multiply_3[0][0]                 \n","                                                                 conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 56, 56, 128)  147584      multiply_4[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 56, 56, 128)  512         conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)       (None, 56, 56, 128)  0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 56, 56, 128)  147584      leaky_re_lu_3[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 56, 56, 128)  512         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 56, 56, 128)  0           multiply_4[0][0]                 \n","                                                                 batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)       (None, 56, 56, 128)  0           add_4[0][0]                      \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 28, 28, 128)  0           leaky_re_lu_4[0][0]              \n","__________________________________________________________________________________________________\n","up_sampling2d_1 (UpSampling2D)  (None, 224, 224, 32) 0           max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","up_sampling2d_2 (UpSampling2D)  (None, 224, 224, 64) 0           max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","up_sampling2d_3 (UpSampling2D)  (None, 224, 224, 128 0           max_pooling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 224, 224, 224 0           up_sampling2d_1[0][0]            \n","                                                                 up_sampling2d_2[0][0]            \n","                                                                 up_sampling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","global_average_pooling2d_3 (Glo (None, 224)          0           concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 256)          57600       global_average_pooling2d_3[0][0] \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 256)          1024        dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 256)          0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 256)          0           activation_7[0][0]               \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 256)          65792       dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 256)          1024        dense_6[0][0]                    \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 256)          0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","dense_7 (Dense)                 (None, 3)            771         activation_8[0][0]               \n","==================================================================================================\n","Total params: 605,823\n","Trainable params: 603,519\n","Non-trainable params: 2,304\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MSHMA1gtMQ6e","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"52cd0935-c1a5-4db0-b9ab-bc1c5e79e915"},"source":["kf = KFold(n_splits=N_SPLITS, random_state=SEED, shuffle=True)\n","\n","for ix, (train_index, test_index) in enumerate(kf.split(range(len(dataset.split_train_test(\"train\")[0])))):\n","                                               \n","    tg = DATASET(SHAPE, BATCH_SIZE, train_index, BASE_DIR, SEED, TRAIN_TEST_RATIO, augment=True)\n","    vg = DATASET(SHAPE, BATCH_SIZE, test_index , BASE_DIR, SEED, TRAIN_TEST_RATIO, augment=False)\n","        \n","    schedule = SGDRScheduler(min_lr=1e-5,            # 1e-6 oldu\n","                             max_lr=1e-2,      # 1e-2 oldu\n","                             steps_per_epoch=np.ceil(EPOCHS/BATCH_SIZE),\n","                             lr_decay=0.9,  #0.9 oldu\n","                             cycle_length=5,  #10 oldu\n","                             mult_factor=1.5)   #2. oldu\n","                                                              #adam(lr=1e-2) oldu\n","    model.compile(loss='categorical_crossentropy', optimizer=adam(lr=1e-2), metrics=[precision, recall, f1, 'acc'])\n","\n","    model_ckpt = \"BRAIN_TUMOR_FOLD_\"+str(ix)+\".h5\"\n","    callbacks = [ModelCheckpoint(model_ckpt, monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=False),\n","                 TensorBoard(log_dir='./log_'+str(ix), update_freq='batch'), \n","                 schedule] \n","                                               \n","    model.fit_generator(tg.data_generator(),\n","                        steps_per_epoch=len(train_index)//BATCH_SIZE,\n","                        epochs=EPOCHS,\n","                        verbose=2,\n","                        validation_data=vg.data_generator(),\n","                        validation_steps=len(test_index)//BATCH_SIZE,\n","                        callbacks=callbacks)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n"," - 422s - loss: 0.9901 - precision: 0.6322 - recall: 0.5415 - f1: 0.5824 - acc: 0.5986 - val_loss: 5.2391 - val_precision: 0.5182 - val_recall: 0.5182 - val_f1: 0.5182 - val_acc: 0.5182\n","\n","Epoch 00001: val_loss improved from inf to 5.23908, saving model to BRAIN_TUMOR_FOLD_0.h5\n","Epoch 2/100\n"," - 187s - loss: 0.7532 - precision: 0.7441 - recall: 0.5565 - f1: 0.6345 - acc: 0.6617 - val_loss: 0.8078 - val_precision: 0.6818 - val_recall: 0.5625 - val_f1: 0.6158 - val_acc: 0.6172\n","\n","Epoch 00002: val_loss improved from 5.23908 to 0.80777, saving model to BRAIN_TUMOR_FOLD_0.h5\n","Epoch 3/100\n"," - 107s - loss: 0.7172 - precision: 0.7405 - recall: 0.6280 - f1: 0.6793 - acc: 0.7013 - val_loss: 0.9891 - val_precision: 0.6217 - val_recall: 0.5130 - val_f1: 0.5619 - val_acc: 0.5599\n","\n","Epoch 00003: val_loss did not improve from 0.80777\n","Epoch 4/100\n"," - 73s - loss: 0.6549 - precision: 0.7717 - recall: 0.6653 - f1: 0.7138 - acc: 0.7188 - val_loss: 1.4898 - val_precision: 0.5272 - val_recall: 0.5182 - val_f1: 0.5226 - val_acc: 0.5182\n","\n","Epoch 00004: val_loss did not improve from 0.80777\n","Epoch 5/100\n"," - 57s - loss: 0.5480 - precision: 0.7938 - recall: 0.7127 - f1: 0.7507 - acc: 0.7632 - val_loss: 1.1399 - val_precision: 0.5046 - val_recall: 0.3177 - val_f1: 0.3898 - val_acc: 0.3594\n","\n","Epoch 00005: val_loss did not improve from 0.80777\n","Epoch 6/100\n"," - 53s - loss: 0.5594 - precision: 0.7800 - recall: 0.7175 - f1: 0.7473 - acc: 0.7542 - val_loss: 0.5548 - val_precision: 0.7977 - val_recall: 0.7109 - val_f1: 0.7517 - val_acc: 0.7474\n","\n","Epoch 00006: val_loss improved from 0.80777 to 0.55478, saving model to BRAIN_TUMOR_FOLD_0.h5\n","Epoch 7/100\n"," - 49s - loss: 0.5587 - precision: 0.7968 - recall: 0.7278 - f1: 0.7604 - acc: 0.7662 - val_loss: 0.8719 - val_precision: 0.6709 - val_recall: 0.4167 - val_f1: 0.5136 - val_acc: 0.6094\n","\n","Epoch 00007: val_loss did not improve from 0.55478\n","Epoch 8/100\n"," - 49s - loss: 0.5004 - precision: 0.8080 - recall: 0.7614 - f1: 0.7838 - acc: 0.7891 - val_loss: 1.3404 - val_precision: 0.5050 - val_recall: 0.4375 - val_f1: 0.4687 - val_acc: 0.5078\n","\n","Epoch 00008: val_loss did not improve from 0.55478\n","Epoch 9/100\n"," - 50s - loss: 0.5033 - precision: 0.8046 - recall: 0.7548 - f1: 0.7786 - acc: 0.7849 - val_loss: 0.9615 - val_precision: 0.6704 - val_recall: 0.6198 - val_f1: 0.6440 - val_acc: 0.6328\n","\n","Epoch 00009: val_loss did not improve from 0.55478\n","Epoch 10/100\n"," - 49s - loss: 0.4725 - precision: 0.8144 - recall: 0.7782 - f1: 0.7958 - acc: 0.7975 - val_loss: 1.7665 - val_precision: 0.5651 - val_recall: 0.5651 - val_f1: 0.5651 - val_acc: 0.5651\n","\n","Epoch 00010: val_loss did not improve from 0.55478\n","Epoch 11/100\n"," - 49s - loss: 0.4362 - precision: 0.8382 - recall: 0.8023 - f1: 0.8197 - acc: 0.8209 - val_loss: 5.6978 - val_precision: 0.5130 - val_recall: 0.5130 - val_f1: 0.5130 - val_acc: 0.5130\n","\n","Epoch 00011: val_loss did not improve from 0.55478\n","Epoch 12/100\n"," - 49s - loss: 0.4692 - precision: 0.8260 - recall: 0.7837 - f1: 0.8041 - acc: 0.8071 - val_loss: 0.8320 - val_precision: 0.7449 - val_recall: 0.4635 - val_f1: 0.5703 - val_acc: 0.6302\n","\n","Epoch 00012: val_loss did not improve from 0.55478\n","Epoch 13/100\n"," - 49s - loss: 0.4007 - precision: 0.8509 - recall: 0.8191 - f1: 0.8346 - acc: 0.8383 - val_loss: 1.2288 - val_precision: 0.4743 - val_recall: 0.3516 - val_f1: 0.4035 - val_acc: 0.4557\n","\n","Epoch 00013: val_loss did not improve from 0.55478\n","Epoch 14/100\n"," - 49s - loss: 0.4362 - precision: 0.8448 - recall: 0.8119 - f1: 0.8279 - acc: 0.8329 - val_loss: 1.4857 - val_precision: 0.5203 - val_recall: 0.2604 - val_f1: 0.3462 - val_acc: 0.3125\n","\n","Epoch 00014: val_loss did not improve from 0.55478\n","Epoch 15/100\n"," - 49s - loss: 0.3857 - precision: 0.8551 - recall: 0.8281 - f1: 0.8413 - acc: 0.8431 - val_loss: 1.2015 - val_precision: 0.5453 - val_recall: 0.5365 - val_f1: 0.5408 - val_acc: 0.5443\n","\n","Epoch 00015: val_loss did not improve from 0.55478\n","Epoch 16/100\n"," - 49s - loss: 0.3890 - precision: 0.8466 - recall: 0.8239 - f1: 0.8350 - acc: 0.8359 - val_loss: 6.6040 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n","\n","Epoch 00016: val_loss did not improve from 0.55478\n","Epoch 17/100\n"," - 49s - loss: 0.3236 - precision: 0.8798 - recall: 0.8618 - f1: 0.8706 - acc: 0.8714 - val_loss: 4.6038 - val_precision: 0.5260 - val_recall: 0.5260 - val_f1: 0.5260 - val_acc: 0.5260\n","\n","Epoch 00017: val_loss did not improve from 0.55478\n","Epoch 18/100\n"," - 49s - loss: 0.2741 - precision: 0.9020 - recall: 0.8894 - f1: 0.8956 - acc: 0.8942 - val_loss: 3.3619 - val_precision: 0.5703 - val_recall: 0.5703 - val_f1: 0.5703 - val_acc: 0.5703\n","\n","Epoch 00018: val_loss did not improve from 0.55478\n","Epoch 19/100\n"," - 49s - loss: 0.3191 - precision: 0.8787 - recall: 0.8624 - f1: 0.8704 - acc: 0.8696 - val_loss: 1.0661 - val_precision: 0.7280 - val_recall: 0.6875 - val_f1: 0.7071 - val_acc: 0.7005\n","\n","Epoch 00019: val_loss did not improve from 0.55478\n","Epoch 20/100\n"," - 49s - loss: 0.3046 - precision: 0.8963 - recall: 0.8774 - f1: 0.8867 - acc: 0.8888 - val_loss: 0.9510 - val_precision: 0.7154 - val_recall: 0.7057 - val_f1: 0.7105 - val_acc: 0.7083\n","\n","Epoch 00020: val_loss did not improve from 0.55478\n","Epoch 21/100\n"," - 49s - loss: 0.3334 - precision: 0.8812 - recall: 0.8672 - f1: 0.8741 - acc: 0.8714 - val_loss: 1.3374 - val_precision: 0.6633 - val_recall: 0.6562 - val_f1: 0.6598 - val_acc: 0.6615\n","\n","Epoch 00021: val_loss did not improve from 0.55478\n","Epoch 22/100\n"," - 49s - loss: 0.2755 - precision: 0.8938 - recall: 0.8792 - f1: 0.8864 - acc: 0.8828 - val_loss: 2.8002 - val_precision: 0.3570 - val_recall: 0.3464 - val_f1: 0.3516 - val_acc: 0.3594\n","\n","Epoch 00022: val_loss did not improve from 0.55478\n","Epoch 23/100\n"," - 49s - loss: 0.2973 - precision: 0.8934 - recall: 0.8810 - f1: 0.8871 - acc: 0.8876 - val_loss: 3.2419 - val_precision: 0.5187 - val_recall: 0.5156 - val_f1: 0.5171 - val_acc: 0.5156\n","\n","Epoch 00023: val_loss did not improve from 0.55478\n","Epoch 24/100\n"," - 49s - loss: 0.2247 - precision: 0.9213 - recall: 0.9093 - f1: 0.9152 - acc: 0.9153 - val_loss: 5.6754 - val_precision: 0.3118 - val_recall: 0.3021 - val_f1: 0.3068 - val_acc: 0.3047\n","\n","Epoch 00024: val_loss did not improve from 0.55478\n","Epoch 25/100\n"," - 49s - loss: 0.3332 - precision: 0.8770 - recall: 0.8648 - f1: 0.8708 - acc: 0.8708 - val_loss: 2.7429 - val_precision: 0.4098 - val_recall: 0.4089 - val_f1: 0.4093 - val_acc: 0.4089\n","\n","Epoch 00025: val_loss did not improve from 0.55478\n","Epoch 26/100\n"," - 49s - loss: 0.3188 - precision: 0.8885 - recall: 0.8654 - f1: 0.8767 - acc: 0.8756 - val_loss: 3.3318 - val_precision: 0.5365 - val_recall: 0.5365 - val_f1: 0.5365 - val_acc: 0.5365\n","\n","Epoch 00026: val_loss did not improve from 0.55478\n","Epoch 27/100\n"," - 49s - loss: 0.2478 - precision: 0.9063 - recall: 0.8858 - f1: 0.8958 - acc: 0.8948 - val_loss: 2.0553 - val_precision: 0.6745 - val_recall: 0.6745 - val_f1: 0.6745 - val_acc: 0.6745\n","\n","Epoch 00027: val_loss did not improve from 0.55478\n","Epoch 28/100\n"," - 49s - loss: 0.2257 - precision: 0.9177 - recall: 0.9038 - f1: 0.9107 - acc: 0.9081 - val_loss: 2.9622 - val_precision: 0.4911 - val_recall: 0.4896 - val_f1: 0.4903 - val_acc: 0.4922\n","\n","Epoch 00028: val_loss did not improve from 0.55478\n","Epoch 29/100\n"," - 49s - loss: 0.2167 - precision: 0.9145 - recall: 0.9056 - f1: 0.9100 - acc: 0.9111 - val_loss: 8.3517 - val_precision: 0.2370 - val_recall: 0.2370 - val_f1: 0.2370 - val_acc: 0.2370\n","\n","Epoch 00029: val_loss did not improve from 0.55478\n","Epoch 30/100\n"," - 49s - loss: 0.2125 - precision: 0.9213 - recall: 0.9153 - f1: 0.9182 - acc: 0.9183 - val_loss: 0.9887 - val_precision: 0.6399 - val_recall: 0.6016 - val_f1: 0.6198 - val_acc: 0.6250\n","\n","Epoch 00030: val_loss did not improve from 0.55478\n","Epoch 31/100\n"," - 49s - loss: 0.2598 - precision: 0.8998 - recall: 0.8912 - f1: 0.8955 - acc: 0.8936 - val_loss: 1.1539 - val_precision: 0.6412 - val_recall: 0.6276 - val_f1: 0.6342 - val_acc: 0.6354\n","\n","Epoch 00031: val_loss did not improve from 0.55478\n","Epoch 32/100\n"," - 49s - loss: 0.2476 - precision: 0.9114 - recall: 0.9032 - f1: 0.9073 - acc: 0.9075 - val_loss: 2.7396 - val_precision: 0.5755 - val_recall: 0.5755 - val_f1: 0.5755 - val_acc: 0.5755\n","\n","Epoch 00032: val_loss did not improve from 0.55478\n","Epoch 33/100\n"," - 49s - loss: 0.1945 - precision: 0.9318 - recall: 0.9195 - f1: 0.9255 - acc: 0.9249 - val_loss: 2.1528 - val_precision: 0.6641 - val_recall: 0.6641 - val_f1: 0.6641 - val_acc: 0.6641\n","\n","Epoch 00033: val_loss did not improve from 0.55478\n","Epoch 34/100\n"," - 49s - loss: 0.2321 - precision: 0.9161 - recall: 0.9105 - f1: 0.9132 - acc: 0.9135 - val_loss: 3.3937 - val_precision: 0.4503 - val_recall: 0.4479 - val_f1: 0.4491 - val_acc: 0.4479\n","\n","Epoch 00034: val_loss did not improve from 0.55478\n","Epoch 35/100\n"," - 49s - loss: 0.2056 - precision: 0.9176 - recall: 0.9099 - f1: 0.9137 - acc: 0.9129 - val_loss: 1.2061 - val_precision: 0.6423 - val_recall: 0.6406 - val_f1: 0.6414 - val_acc: 0.6432\n","\n","Epoch 00035: val_loss did not improve from 0.55478\n","Epoch 36/100\n"," - 49s - loss: 0.1906 - precision: 0.9269 - recall: 0.9135 - f1: 0.9201 - acc: 0.9165 - val_loss: 1.5121 - val_precision: 0.5974 - val_recall: 0.5911 - val_f1: 0.5943 - val_acc: 0.5964\n","\n","Epoch 00036: val_loss did not improve from 0.55478\n","Epoch 37/100\n"," - 49s - loss: 0.1769 - precision: 0.9338 - recall: 0.9219 - f1: 0.9277 - acc: 0.9261 - val_loss: 0.7042 - val_precision: 0.7939 - val_recall: 0.7917 - val_f1: 0.7928 - val_acc: 0.7943\n","\n","Epoch 00037: val_loss did not improve from 0.55478\n","Epoch 38/100\n"," - 50s - loss: 0.1479 - precision: 0.9431 - recall: 0.9357 - f1: 0.9394 - acc: 0.9387 - val_loss: 1.2660 - val_precision: 0.7214 - val_recall: 0.7214 - val_f1: 0.7214 - val_acc: 0.7214\n","\n","Epoch 00038: val_loss did not improve from 0.55478\n","Epoch 39/100\n"," - 49s - loss: 0.2189 - precision: 0.9216 - recall: 0.9111 - f1: 0.9163 - acc: 0.9165 - val_loss: 3.1664 - val_precision: 0.5781 - val_recall: 0.5781 - val_f1: 0.5781 - val_acc: 0.5781\n","\n","Epoch 00039: val_loss did not improve from 0.55478\n","Epoch 40/100\n"," - 49s - loss: 0.1969 - precision: 0.9210 - recall: 0.9044 - f1: 0.9126 - acc: 0.9123 - val_loss: 2.3236 - val_precision: 0.5094 - val_recall: 0.5026 - val_f1: 0.5060 - val_acc: 0.5026\n","\n","Epoch 00040: val_loss did not improve from 0.55478\n","Epoch 41/100\n"," - 49s - loss: 0.1794 - precision: 0.9375 - recall: 0.9273 - f1: 0.9323 - acc: 0.9315 - val_loss: 1.7526 - val_precision: 0.7550 - val_recall: 0.7474 - val_f1: 0.7511 - val_acc: 0.7578\n","\n","Epoch 00041: val_loss did not improve from 0.55478\n","Epoch 42/100\n"," - 49s - loss: 0.1572 - precision: 0.9450 - recall: 0.9375 - f1: 0.9412 - acc: 0.9411 - val_loss: 3.6949 - val_precision: 0.3610 - val_recall: 0.3516 - val_f1: 0.3562 - val_acc: 0.3594\n","\n","Epoch 00042: val_loss did not improve from 0.55478\n","Epoch 43/100\n"," - 49s - loss: 0.1850 - precision: 0.9359 - recall: 0.9303 - f1: 0.9331 - acc: 0.9321 - val_loss: 2.3945 - val_precision: 0.4656 - val_recall: 0.4609 - val_f1: 0.4633 - val_acc: 0.4661\n","\n","Epoch 00043: val_loss did not improve from 0.55478\n","Epoch 44/100\n"," - 49s - loss: 0.1854 - precision: 0.9389 - recall: 0.9327 - f1: 0.9358 - acc: 0.9363 - val_loss: 1.1271 - val_precision: 0.6817 - val_recall: 0.6693 - val_f1: 0.6754 - val_acc: 0.6693\n","\n","Epoch 00044: val_loss did not improve from 0.55478\n","Epoch 45/100\n"," - 49s - loss: 0.1456 - precision: 0.9446 - recall: 0.9417 - f1: 0.9431 - acc: 0.9423 - val_loss: 0.5931 - val_precision: 0.8102 - val_recall: 0.7995 - val_f1: 0.8048 - val_acc: 0.8021\n","\n","Epoch 00045: val_loss did not improve from 0.55478\n","Epoch 46/100\n"," - 49s - loss: 0.1430 - precision: 0.9456 - recall: 0.9405 - f1: 0.9430 - acc: 0.9411 - val_loss: 0.5808 - val_precision: 0.8409 - val_recall: 0.8255 - val_f1: 0.8331 - val_acc: 0.8255\n","\n","Epoch 00046: val_loss did not improve from 0.55478\n","Epoch 47/100\n"," - 49s - loss: 0.1368 - precision: 0.9491 - recall: 0.9423 - f1: 0.9456 - acc: 0.9441 - val_loss: 1.7586 - val_precision: 0.6062 - val_recall: 0.5964 - val_f1: 0.6012 - val_acc: 0.6016\n","\n","Epoch 00047: val_loss did not improve from 0.55478\n","Epoch 48/100\n"," - 48s - loss: 0.1968 - precision: 0.9323 - recall: 0.9279 - f1: 0.9301 - acc: 0.9297 - val_loss: 9.3648 - val_precision: 0.2995 - val_recall: 0.2995 - val_f1: 0.2995 - val_acc: 0.2995\n","\n","Epoch 00048: val_loss did not improve from 0.55478\n","Epoch 49/100\n"," - 49s - loss: 0.1717 - precision: 0.9343 - recall: 0.9291 - f1: 0.9316 - acc: 0.9315 - val_loss: 0.4225 - val_precision: 0.8204 - val_recall: 0.7943 - val_f1: 0.8070 - val_acc: 0.7969\n","\n","Epoch 00049: val_loss improved from 0.55478 to 0.42251, saving model to BRAIN_TUMOR_FOLD_0.h5\n","Epoch 50/100\n"," - 49s - loss: 0.1301 - precision: 0.9504 - recall: 0.9471 - f1: 0.9487 - acc: 0.9483 - val_loss: 0.3594 - val_precision: 0.8786 - val_recall: 0.8672 - val_f1: 0.8728 - val_acc: 0.8750\n","\n","Epoch 00050: val_loss improved from 0.42251 to 0.35940, saving model to BRAIN_TUMOR_FOLD_0.h5\n","Epoch 51/100\n"," - 49s - loss: 0.1358 - precision: 0.9504 - recall: 0.9441 - f1: 0.9472 - acc: 0.9483 - val_loss: 1.9507 - val_precision: 0.6147 - val_recall: 0.6016 - val_f1: 0.6080 - val_acc: 0.6094\n","\n","Epoch 00051: val_loss did not improve from 0.35940\n","Epoch 52/100\n"," - 49s - loss: 0.2005 - precision: 0.9238 - recall: 0.9177 - f1: 0.9207 - acc: 0.9219 - val_loss: 7.6231 - val_precision: 0.4922 - val_recall: 0.4922 - val_f1: 0.4922 - val_acc: 0.4922\n","\n","Epoch 00052: val_loss did not improve from 0.35940\n","Epoch 53/100\n"," - 49s - loss: 0.1575 - precision: 0.9449 - recall: 0.9375 - f1: 0.9411 - acc: 0.9399 - val_loss: 5.8413 - val_precision: 0.5104 - val_recall: 0.5104 - val_f1: 0.5104 - val_acc: 0.5104\n","\n","Epoch 00053: val_loss did not improve from 0.35940\n","Epoch 54/100\n"," - 49s - loss: 0.1427 - precision: 0.9496 - recall: 0.9405 - f1: 0.9450 - acc: 0.9459 - val_loss: 2.7131 - val_precision: 0.5859 - val_recall: 0.5859 - val_f1: 0.5859 - val_acc: 0.5859\n","\n","Epoch 00054: val_loss did not improve from 0.35940\n","Epoch 55/100\n"," - 49s - loss: 0.1088 - precision: 0.9638 - recall: 0.9603 - f1: 0.9621 - acc: 0.9621 - val_loss: 0.8840 - val_precision: 0.7805 - val_recall: 0.7786 - val_f1: 0.7796 - val_acc: 0.7812\n","\n","Epoch 00055: val_loss did not improve from 0.35940\n","Epoch 56/100\n"," - 49s - loss: 0.1974 - precision: 0.9308 - recall: 0.9225 - f1: 0.9266 - acc: 0.9261 - val_loss: 8.8937 - val_precision: 0.3620 - val_recall: 0.3620 - val_f1: 0.3620 - val_acc: 0.3620\n","\n","Epoch 00056: val_loss did not improve from 0.35940\n","Epoch 57/100\n"," - 49s - loss: 0.1652 - precision: 0.9389 - recall: 0.9321 - f1: 0.9355 - acc: 0.9357 - val_loss: 1.5467 - val_precision: 0.6790 - val_recall: 0.6771 - val_f1: 0.6780 - val_acc: 0.6797\n","\n","Epoch 00057: val_loss did not improve from 0.35940\n","Epoch 58/100\n"," - 48s - loss: 0.1400 - precision: 0.9487 - recall: 0.9441 - f1: 0.9464 - acc: 0.9465 - val_loss: 1.2283 - val_precision: 0.7118 - val_recall: 0.7083 - val_f1: 0.7101 - val_acc: 0.7083\n","\n","Epoch 00058: val_loss did not improve from 0.35940\n","Epoch 59/100\n"," - 49s - loss: 0.1217 - precision: 0.9596 - recall: 0.9555 - f1: 0.9576 - acc: 0.9573 - val_loss: 0.2912 - val_precision: 0.9141 - val_recall: 0.8906 - val_f1: 0.9022 - val_acc: 0.9115\n","\n","Epoch 00059: val_loss improved from 0.35940 to 0.29122, saving model to BRAIN_TUMOR_FOLD_0.h5\n","Epoch 60/100\n"," - 49s - loss: 0.1464 - precision: 0.9498 - recall: 0.9465 - f1: 0.9481 - acc: 0.9477 - val_loss: 3.0864 - val_precision: 0.5460 - val_recall: 0.5391 - val_f1: 0.5425 - val_acc: 0.5469\n","\n","Epoch 00060: val_loss did not improve from 0.29122\n","Epoch 61/100\n"," - 48s - loss: 0.1267 - precision: 0.9572 - recall: 0.9549 - f1: 0.9561 - acc: 0.9555 - val_loss: 3.0402 - val_precision: 0.5077 - val_recall: 0.5052 - val_f1: 0.5065 - val_acc: 0.5078\n","\n","Epoch 00061: val_loss did not improve from 0.29122\n","Epoch 62/100\n"," - 49s - loss: 0.0937 - precision: 0.9705 - recall: 0.9675 - f1: 0.9690 - acc: 0.9694 - val_loss: 1.6503 - val_precision: 0.6092 - val_recall: 0.5964 - val_f1: 0.6026 - val_acc: 0.5990\n","\n","Epoch 00062: val_loss did not improve from 0.29122\n","Epoch 63/100\n"," - 48s - loss: 0.1045 - precision: 0.9614 - recall: 0.9597 - f1: 0.9606 - acc: 0.9603 - val_loss: 1.0059 - val_precision: 0.7706 - val_recall: 0.7448 - val_f1: 0.7574 - val_acc: 0.7682\n","\n","Epoch 00063: val_loss did not improve from 0.29122\n","Epoch 64/100\n"," - 48s - loss: 0.1438 - precision: 0.9481 - recall: 0.9435 - f1: 0.9458 - acc: 0.9465 - val_loss: 2.2144 - val_precision: 0.7786 - val_recall: 0.7786 - val_f1: 0.7786 - val_acc: 0.7786\n","\n","Epoch 00064: val_loss did not improve from 0.29122\n","Epoch 65/100\n"," - 49s - loss: 0.1778 - precision: 0.9335 - recall: 0.9279 - f1: 0.9307 - acc: 0.9291 - val_loss: 0.6078 - val_precision: 0.7669 - val_recall: 0.7526 - val_f1: 0.7597 - val_acc: 0.7656\n","\n","Epoch 00065: val_loss did not improve from 0.29122\n","Epoch 66/100\n"," - 49s - loss: 0.1210 - precision: 0.9534 - recall: 0.9477 - f1: 0.9505 - acc: 0.9525 - val_loss: 0.6311 - val_precision: 0.8063 - val_recall: 0.8021 - val_f1: 0.8042 - val_acc: 0.8047\n","\n","Epoch 00066: val_loss did not improve from 0.29122\n","Epoch 67/100\n"," - 48s - loss: 0.1091 - precision: 0.9645 - recall: 0.9633 - f1: 0.9639 - acc: 0.9639 - val_loss: 1.0846 - val_precision: 0.7630 - val_recall: 0.7630 - val_f1: 0.7630 - val_acc: 0.7630\n","\n","Epoch 00067: val_loss did not improve from 0.29122\n","Epoch 68/100\n"," - 49s - loss: 0.1019 - precision: 0.9603 - recall: 0.9591 - f1: 0.9597 - acc: 0.9597 - val_loss: 2.8685 - val_precision: 0.5964 - val_recall: 0.5964 - val_f1: 0.5964 - val_acc: 0.5964\n","\n","Epoch 00068: val_loss did not improve from 0.29122\n","Epoch 69/100\n"," - 49s - loss: 0.1543 - precision: 0.9452 - recall: 0.9429 - f1: 0.9441 - acc: 0.9441 - val_loss: 11.4925 - val_precision: 0.2422 - val_recall: 0.2422 - val_f1: 0.2422 - val_acc: 0.2422\n","\n","Epoch 00069: val_loss did not improve from 0.29122\n","Epoch 70/100\n"," - 49s - loss: 0.0991 - precision: 0.9662 - recall: 0.9639 - f1: 0.9651 - acc: 0.9645 - val_loss: 8.5453 - val_precision: 0.3438 - val_recall: 0.3438 - val_f1: 0.3437 - val_acc: 0.3438\n","\n","Epoch 00070: val_loss did not improve from 0.29122\n","Epoch 71/100\n"," - 49s - loss: 0.1412 - precision: 0.9548 - recall: 0.9519 - f1: 0.9533 - acc: 0.9537 - val_loss: 2.6372 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n","\n","Epoch 00071: val_loss did not improve from 0.29122\n","Epoch 72/100\n"," - 49s - loss: 0.1169 - precision: 0.9594 - recall: 0.9555 - f1: 0.9575 - acc: 0.9579 - val_loss: 2.0970 - val_precision: 0.5864 - val_recall: 0.5807 - val_f1: 0.5835 - val_acc: 0.5833\n","\n","Epoch 00072: val_loss did not improve from 0.29122\n","Epoch 73/100\n"," - 48s - loss: 0.0827 - precision: 0.9693 - recall: 0.9663 - f1: 0.9678 - acc: 0.9681 - val_loss: 0.5072 - val_precision: 0.8636 - val_recall: 0.8568 - val_f1: 0.8602 - val_acc: 0.8646\n","\n","Epoch 00073: val_loss did not improve from 0.29122\n","Epoch 74/100\n"," - 49s - loss: 0.0839 - precision: 0.9723 - recall: 0.9700 - f1: 0.9711 - acc: 0.9712 - val_loss: 0.2129 - val_precision: 0.9318 - val_recall: 0.9245 - val_f1: 0.9281 - val_acc: 0.9271\n","\n","Epoch 00074: val_loss improved from 0.29122 to 0.21294, saving model to BRAIN_TUMOR_FOLD_0.h5\n","Epoch 75/100\n"," - 49s - loss: 0.0765 - precision: 0.9789 - recall: 0.9766 - f1: 0.9777 - acc: 0.9784 - val_loss: 0.7634 - val_precision: 0.8302 - val_recall: 0.8281 - val_f1: 0.8292 - val_acc: 0.8281\n","\n","Epoch 00075: val_loss did not improve from 0.21294\n","Epoch 76/100\n"," - 49s - loss: 0.1235 - precision: 0.9603 - recall: 0.9579 - f1: 0.9591 - acc: 0.9585 - val_loss: 2.8510 - val_precision: 0.5690 - val_recall: 0.5677 - val_f1: 0.5683 - val_acc: 0.5703\n","\n","Epoch 00076: val_loss did not improve from 0.21294\n","Epoch 77/100\n"," - 49s - loss: 0.1443 - precision: 0.9523 - recall: 0.9495 - f1: 0.9509 - acc: 0.9501 - val_loss: 0.2156 - val_precision: 0.9033 - val_recall: 0.9010 - val_f1: 0.9022 - val_acc: 0.9036\n","\n","Epoch 00077: val_loss did not improve from 0.21294\n","Epoch 78/100\n"," - 49s - loss: 0.1225 - precision: 0.9554 - recall: 0.9519 - f1: 0.9536 - acc: 0.9531 - val_loss: 2.2577 - val_precision: 0.6276 - val_recall: 0.6276 - val_f1: 0.6276 - val_acc: 0.6276\n","\n","Epoch 00078: val_loss did not improve from 0.21294\n","Epoch 79/100\n"," - 49s - loss: 0.0942 - precision: 0.9661 - recall: 0.9609 - f1: 0.9635 - acc: 0.9627 - val_loss: 4.3021 - val_precision: 0.5052 - val_recall: 0.5052 - val_f1: 0.5052 - val_acc: 0.5052\n","\n","Epoch 00079: val_loss did not improve from 0.21294\n","Epoch 80/100\n"," - 48s - loss: 0.0785 - precision: 0.9722 - recall: 0.9675 - f1: 0.9699 - acc: 0.9712 - val_loss: 2.9236 - val_precision: 0.5677 - val_recall: 0.5677 - val_f1: 0.5677 - val_acc: 0.5677\n","\n","Epoch 00080: val_loss did not improve from 0.21294\n","Epoch 81/100\n"," - 49s - loss: 0.0667 - precision: 0.9759 - recall: 0.9748 - f1: 0.9753 - acc: 0.9760 - val_loss: 1.2892 - val_precision: 0.6982 - val_recall: 0.6927 - val_f1: 0.6955 - val_acc: 0.6953\n","\n","Epoch 00081: val_loss did not improve from 0.21294\n","Epoch 82/100\n"," - 49s - loss: 0.1045 - precision: 0.9638 - recall: 0.9591 - f1: 0.9614 - acc: 0.9615 - val_loss: 0.8397 - val_precision: 0.7875 - val_recall: 0.7734 - val_f1: 0.7804 - val_acc: 0.7760\n","\n","Epoch 00082: val_loss did not improve from 0.21294\n","Epoch 83/100\n"," - 49s - loss: 0.1111 - precision: 0.9614 - recall: 0.9573 - f1: 0.9593 - acc: 0.9603 - val_loss: 2.3956 - val_precision: 0.5833 - val_recall: 0.5833 - val_f1: 0.5833 - val_acc: 0.5833\n","\n","Epoch 00083: val_loss did not improve from 0.21294\n","Epoch 84/100\n"," - 49s - loss: 0.1324 - precision: 0.9524 - recall: 0.9483 - f1: 0.9503 - acc: 0.9495 - val_loss: 0.8796 - val_precision: 0.7927 - val_recall: 0.7865 - val_f1: 0.7896 - val_acc: 0.7917\n","\n","Epoch 00084: val_loss did not improve from 0.21294\n","Epoch 85/100\n"," - 49s - loss: 0.1076 - precision: 0.9583 - recall: 0.9519 - f1: 0.9551 - acc: 0.9555 - val_loss: 0.4419 - val_precision: 0.8520 - val_recall: 0.8385 - val_f1: 0.8452 - val_acc: 0.8542\n","\n","Epoch 00085: val_loss did not improve from 0.21294\n","Epoch 86/100\n"," - 49s - loss: 0.0738 - precision: 0.9736 - recall: 0.9736 - f1: 0.9736 - acc: 0.9736 - val_loss: 0.3348 - val_precision: 0.8945 - val_recall: 0.8854 - val_f1: 0.8899 - val_acc: 0.8880\n","\n","Epoch 00086: val_loss did not improve from 0.21294\n","Epoch 87/100\n"," - 49s - loss: 0.0717 - precision: 0.9729 - recall: 0.9712 - f1: 0.9720 - acc: 0.9712 - val_loss: 0.1424 - val_precision: 0.9401 - val_recall: 0.9401 - val_f1: 0.9401 - val_acc: 0.9401\n","\n","Epoch 00087: val_loss improved from 0.21294 to 0.14235, saving model to BRAIN_TUMOR_FOLD_0.h5\n","Epoch 88/100\n"," - 49s - loss: 0.0815 - precision: 0.9753 - recall: 0.9730 - f1: 0.9741 - acc: 0.9736 - val_loss: 1.3670 - val_precision: 0.6781 - val_recall: 0.6745 - val_f1: 0.6763 - val_acc: 0.6745\n","\n","Epoch 00088: val_loss did not improve from 0.14235\n","Epoch 89/100\n"," - 49s - loss: 0.1069 - precision: 0.9619 - recall: 0.9585 - f1: 0.9602 - acc: 0.9603 - val_loss: 8.4519 - val_precision: 0.3307 - val_recall: 0.3307 - val_f1: 0.3307 - val_acc: 0.3307\n","\n","Epoch 00089: val_loss did not improve from 0.14235\n","Epoch 90/100\n"," - 49s - loss: 0.1260 - precision: 0.9537 - recall: 0.9513 - f1: 0.9525 - acc: 0.9525 - val_loss: 1.1991 - val_precision: 0.6163 - val_recall: 0.6146 - val_f1: 0.6154 - val_acc: 0.6172\n","\n","Epoch 00090: val_loss did not improve from 0.14235\n","Epoch 91/100\n"," - 49s - loss: 0.1210 - precision: 0.9597 - recall: 0.9579 - f1: 0.9588 - acc: 0.9591 - val_loss: 1.3455 - val_precision: 0.7467 - val_recall: 0.7448 - val_f1: 0.7458 - val_acc: 0.7474\n","\n","Epoch 00091: val_loss did not improve from 0.14235\n","Epoch 92/100\n"," - 49s - loss: 0.0909 - precision: 0.9723 - recall: 0.9694 - f1: 0.9708 - acc: 0.9706 - val_loss: 0.8610 - val_precision: 0.7417 - val_recall: 0.7318 - val_f1: 0.7367 - val_acc: 0.7448\n","\n","Epoch 00092: val_loss did not improve from 0.14235\n","Epoch 93/100\n"," - 49s - loss: 0.0752 - precision: 0.9772 - recall: 0.9760 - f1: 0.9766 - acc: 0.9772 - val_loss: 0.3195 - val_precision: 0.8587 - val_recall: 0.8542 - val_f1: 0.8564 - val_acc: 0.8594\n","\n","Epoch 00093: val_loss did not improve from 0.14235\n","Epoch 94/100\n"," - 49s - loss: 0.0480 - precision: 0.9843 - recall: 0.9832 - f1: 0.9838 - acc: 0.9838 - val_loss: 4.2448 - val_precision: 0.4377 - val_recall: 0.4323 - val_f1: 0.4349 - val_acc: 0.4323\n","\n","Epoch 00094: val_loss did not improve from 0.14235\n","Epoch 95/100\n"," - 49s - loss: 0.1045 - precision: 0.9615 - recall: 0.9609 - f1: 0.9612 - acc: 0.9615 - val_loss: 2.2071 - val_precision: 0.5663 - val_recall: 0.5339 - val_f1: 0.5496 - val_acc: 0.5495\n","\n","Epoch 00095: val_loss did not improve from 0.14235\n","Epoch 96/100\n"," - 48s - loss: 0.1209 - precision: 0.9523 - recall: 0.9501 - f1: 0.9512 - acc: 0.9519 - val_loss: 0.5494 - val_precision: 0.8508 - val_recall: 0.8464 - val_f1: 0.8486 - val_acc: 0.8490\n","\n","Epoch 00096: val_loss did not improve from 0.14235\n","Epoch 97/100\n"," - 49s - loss: 0.1220 - precision: 0.9523 - recall: 0.9483 - f1: 0.9503 - acc: 0.9495 - val_loss: 4.5378 - val_precision: 0.5807 - val_recall: 0.5807 - val_f1: 0.5807 - val_acc: 0.5807\n","\n","Epoch 00097: val_loss did not improve from 0.14235\n","Epoch 98/100\n"," - 49s - loss: 0.0782 - precision: 0.9729 - recall: 0.9706 - f1: 0.9717 - acc: 0.9712 - val_loss: 0.1100 - val_precision: 0.9551 - val_recall: 0.9453 - val_f1: 0.9501 - val_acc: 0.9453\n","\n","Epoch 00098: val_loss improved from 0.14235 to 0.11000, saving model to BRAIN_TUMOR_FOLD_0.h5\n","Epoch 99/100\n"," - 48s - loss: 0.0812 - precision: 0.9753 - recall: 0.9724 - f1: 0.9738 - acc: 0.9736 - val_loss: 0.2082 - val_precision: 0.9136 - val_recall: 0.9089 - val_f1: 0.9112 - val_acc: 0.9141\n","\n","Epoch 00099: val_loss did not improve from 0.11000\n","Epoch 100/100\n"," - 49s - loss: 0.0550 - precision: 0.9849 - recall: 0.9826 - f1: 0.9837 - acc: 0.9838 - val_loss: 0.8004 - val_precision: 0.7708 - val_recall: 0.7604 - val_f1: 0.7655 - val_acc: 0.7682\n","\n","Epoch 00100: val_loss did not improve from 0.11000\n","Epoch 1/100\n"," - 51s - loss: 0.3144 - precision: 0.8946 - recall: 0.8840 - f1: 0.8892 - acc: 0.8888 - val_loss: 3.5224 - val_precision: 0.4453 - val_recall: 0.4453 - val_f1: 0.4453 - val_acc: 0.4453\n","\n","Epoch 00001: val_loss improved from inf to 3.52243, saving model to BRAIN_TUMOR_FOLD_1.h5\n","Epoch 2/100\n"," - 47s - loss: 0.2269 - precision: 0.9071 - recall: 0.8984 - f1: 0.9027 - acc: 0.9020 - val_loss: 3.2609 - val_precision: 0.3823 - val_recall: 0.3776 - val_f1: 0.3799 - val_acc: 0.3828\n","\n","Epoch 00002: val_loss improved from 3.52243 to 3.26086, saving model to BRAIN_TUMOR_FOLD_1.h5\n","Epoch 3/100\n"," - 49s - loss: 0.1956 - precision: 0.9230 - recall: 0.9147 - f1: 0.9188 - acc: 0.9195 - val_loss: 1.3278 - val_precision: 0.6745 - val_recall: 0.6589 - val_f1: 0.6665 - val_acc: 0.6771\n","\n","Epoch 00003: val_loss improved from 3.26086 to 1.32778, saving model to BRAIN_TUMOR_FOLD_1.h5\n","Epoch 4/100\n"," - 48s - loss: 0.1525 - precision: 0.9443 - recall: 0.9381 - f1: 0.9412 - acc: 0.9411 - val_loss: 6.5575 - val_precision: 0.3906 - val_recall: 0.3906 - val_f1: 0.3906 - val_acc: 0.3906\n","\n","Epoch 00004: val_loss did not improve from 1.32778\n","Epoch 5/100\n"," - 48s - loss: 0.1716 - precision: 0.9359 - recall: 0.9285 - f1: 0.9321 - acc: 0.9309 - val_loss: 11.0490 - val_precision: 0.3125 - val_recall: 0.3125 - val_f1: 0.3125 - val_acc: 0.3125\n","\n","Epoch 00005: val_loss did not improve from 1.32778\n","Epoch 6/100\n"," - 49s - loss: 0.1814 - precision: 0.9312 - recall: 0.9201 - f1: 0.9256 - acc: 0.9267 - val_loss: 0.8468 - val_precision: 0.7333 - val_recall: 0.7083 - val_f1: 0.7206 - val_acc: 0.7396\n","\n","Epoch 00006: val_loss improved from 1.32778 to 0.84685, saving model to BRAIN_TUMOR_FOLD_1.h5\n","Epoch 7/100\n"," - 48s - loss: 0.1843 - precision: 0.9363 - recall: 0.9291 - f1: 0.9327 - acc: 0.9321 - val_loss: 10.7980 - val_precision: 0.3229 - val_recall: 0.3229 - val_f1: 0.3229 - val_acc: 0.3229\n","\n","Epoch 00007: val_loss did not improve from 0.84685\n","Epoch 8/100\n"," - 49s - loss: 0.1618 - precision: 0.9469 - recall: 0.9423 - f1: 0.9446 - acc: 0.9441 - val_loss: 0.3723 - val_precision: 0.8840 - val_recall: 0.8724 - val_f1: 0.8781 - val_acc: 0.8776\n","\n","Epoch 00008: val_loss improved from 0.84685 to 0.37230, saving model to BRAIN_TUMOR_FOLD_1.h5\n","Epoch 9/100\n"," - 48s - loss: 0.1450 - precision: 0.9481 - recall: 0.9441 - f1: 0.9461 - acc: 0.9459 - val_loss: 5.7406 - val_precision: 0.4792 - val_recall: 0.4792 - val_f1: 0.4792 - val_acc: 0.4792\n","\n","Epoch 00009: val_loss did not improve from 0.37230\n","Epoch 10/100\n"," - 48s - loss: 0.1582 - precision: 0.9380 - recall: 0.9357 - f1: 0.9368 - acc: 0.9369 - val_loss: 8.9249 - val_precision: 0.2456 - val_recall: 0.2448 - val_f1: 0.2452 - val_acc: 0.2474\n","\n","Epoch 00010: val_loss did not improve from 0.37230\n","Epoch 11/100\n"," - 49s - loss: 0.1686 - precision: 0.9340 - recall: 0.9297 - f1: 0.9318 - acc: 0.9315 - val_loss: 1.4127 - val_precision: 0.6341 - val_recall: 0.6276 - val_f1: 0.6308 - val_acc: 0.6328\n","\n","Epoch 00011: val_loss did not improve from 0.37230\n","Epoch 12/100\n"," - 48s - loss: 0.1521 - precision: 0.9481 - recall: 0.9441 - f1: 0.9461 - acc: 0.9453 - val_loss: 0.7595 - val_precision: 0.7500 - val_recall: 0.7500 - val_f1: 0.7500 - val_acc: 0.7500\n","\n","Epoch 00012: val_loss did not improve from 0.37230\n","Epoch 13/100\n"," - 48s - loss: 0.1366 - precision: 0.9438 - recall: 0.9411 - f1: 0.9425 - acc: 0.9411 - val_loss: 2.0279 - val_precision: 0.6094 - val_recall: 0.6094 - val_f1: 0.6094 - val_acc: 0.6094\n","\n","Epoch 00013: val_loss did not improve from 0.37230\n","Epoch 14/100\n"," - 48s - loss: 0.1318 - precision: 0.9570 - recall: 0.9501 - f1: 0.9535 - acc: 0.9531 - val_loss: 1.9270 - val_precision: 0.6615 - val_recall: 0.6615 - val_f1: 0.6615 - val_acc: 0.6615\n","\n","Epoch 00014: val_loss did not improve from 0.37230\n","Epoch 15/100\n"," - 48s - loss: 0.1500 - precision: 0.9447 - recall: 0.9339 - f1: 0.9392 - acc: 0.9387 - val_loss: 0.2268 - val_precision: 0.9191 - val_recall: 0.9167 - val_f1: 0.9179 - val_acc: 0.9167\n","\n","Epoch 00015: val_loss improved from 0.37230 to 0.22678, saving model to BRAIN_TUMOR_FOLD_1.h5\n","Epoch 16/100\n"," - 48s - loss: 0.1383 - precision: 0.9528 - recall: 0.9453 - f1: 0.9490 - acc: 0.9483 - val_loss: 1.2641 - val_precision: 0.6732 - val_recall: 0.6589 - val_f1: 0.6659 - val_acc: 0.6797\n","\n","Epoch 00016: val_loss did not improve from 0.22678\n","Epoch 17/100\n"," - 49s - loss: 0.1192 - precision: 0.9607 - recall: 0.9561 - f1: 0.9584 - acc: 0.9591 - val_loss: 4.4561 - val_precision: 0.5156 - val_recall: 0.5156 - val_f1: 0.5156 - val_acc: 0.5156\n","\n","Epoch 00017: val_loss did not improve from 0.22678\n","Epoch 18/100\n"," - 48s - loss: 0.0787 - precision: 0.9747 - recall: 0.9724 - f1: 0.9735 - acc: 0.9730 - val_loss: 2.8293 - val_precision: 0.6250 - val_recall: 0.6250 - val_f1: 0.6250 - val_acc: 0.6250\n","\n","Epoch 00018: val_loss did not improve from 0.22678\n","Epoch 19/100\n"," - 49s - loss: 0.1449 - precision: 0.9451 - recall: 0.9423 - f1: 0.9437 - acc: 0.9429 - val_loss: 1.0838 - val_precision: 0.7156 - val_recall: 0.6953 - val_f1: 0.7052 - val_acc: 0.7031\n","\n","Epoch 00019: val_loss did not improve from 0.22678\n","Epoch 20/100\n"," - 49s - loss: 0.1307 - precision: 0.9551 - recall: 0.9465 - f1: 0.9507 - acc: 0.9501 - val_loss: 0.6352 - val_precision: 0.7884 - val_recall: 0.7786 - val_f1: 0.7835 - val_acc: 0.7891\n","\n","Epoch 00020: val_loss did not improve from 0.22678\n","Epoch 21/100\n"," - 49s - loss: 0.1589 - precision: 0.9419 - recall: 0.9369 - f1: 0.9394 - acc: 0.9387 - val_loss: 2.8517 - val_precision: 0.5370 - val_recall: 0.5312 - val_f1: 0.5341 - val_acc: 0.5312\n","\n","Epoch 00021: val_loss did not improve from 0.22678\n","Epoch 22/100\n"," - 49s - loss: 0.1170 - precision: 0.9578 - recall: 0.9549 - f1: 0.9563 - acc: 0.9555 - val_loss: 0.7369 - val_precision: 0.7593 - val_recall: 0.7552 - val_f1: 0.7573 - val_acc: 0.7604\n","\n","Epoch 00022: val_loss did not improve from 0.22678\n","Epoch 23/100\n"," - 50s - loss: 0.0909 - precision: 0.9680 - recall: 0.9651 - f1: 0.9666 - acc: 0.9663 - val_loss: 0.8644 - val_precision: 0.7789 - val_recall: 0.7708 - val_f1: 0.7748 - val_acc: 0.7734\n","\n","Epoch 00023: val_loss did not improve from 0.22678\n","Epoch 24/100\n"," - 49s - loss: 0.1109 - precision: 0.9644 - recall: 0.9615 - f1: 0.9630 - acc: 0.9621 - val_loss: 3.8861 - val_precision: 0.5590 - val_recall: 0.5573 - val_f1: 0.5581 - val_acc: 0.5573\n","\n","Epoch 00024: val_loss did not improve from 0.22678\n","Epoch 25/100\n"," - 49s - loss: 0.1342 - precision: 0.9518 - recall: 0.9501 - f1: 0.9510 - acc: 0.9507 - val_loss: 3.2096 - val_precision: 0.5026 - val_recall: 0.5026 - val_f1: 0.5026 - val_acc: 0.5026\n","\n","Epoch 00025: val_loss did not improve from 0.22678\n","Epoch 26/100\n"," - 49s - loss: 0.1246 - precision: 0.9615 - recall: 0.9597 - f1: 0.9606 - acc: 0.9615 - val_loss: 4.7316 - val_precision: 0.5469 - val_recall: 0.5469 - val_f1: 0.5469 - val_acc: 0.5469\n","\n","Epoch 00026: val_loss did not improve from 0.22678\n","Epoch 27/100\n"," - 49s - loss: 0.0952 - precision: 0.9681 - recall: 0.9669 - f1: 0.9675 - acc: 0.9675 - val_loss: 0.1608 - val_precision: 0.9346 - val_recall: 0.9323 - val_f1: 0.9335 - val_acc: 0.9323\n","\n","Epoch 00027: val_loss improved from 0.22678 to 0.16083, saving model to BRAIN_TUMOR_FOLD_1.h5\n","Epoch 28/100\n"," - 49s - loss: 0.0700 - precision: 0.9765 - recall: 0.9742 - f1: 0.9753 - acc: 0.9754 - val_loss: 0.6518 - val_precision: 0.8691 - val_recall: 0.8646 - val_f1: 0.8668 - val_acc: 0.8698\n","\n","Epoch 00028: val_loss did not improve from 0.16083\n","Epoch 29/100\n"," - 49s - loss: 0.1252 - precision: 0.9578 - recall: 0.9555 - f1: 0.9566 - acc: 0.9573 - val_loss: 0.1733 - val_precision: 0.9686 - val_recall: 0.9635 - val_f1: 0.9661 - val_acc: 0.9661\n","\n","Epoch 00029: val_loss did not improve from 0.16083\n","Epoch 30/100\n"," - 49s - loss: 0.0783 - precision: 0.9735 - recall: 0.9706 - f1: 0.9720 - acc: 0.9718 - val_loss: 4.7850 - val_precision: 0.2474 - val_recall: 0.2474 - val_f1: 0.2474 - val_acc: 0.2474\n","\n","Epoch 00030: val_loss did not improve from 0.16083\n","Epoch 31/100\n"," - 48s - loss: 0.0844 - precision: 0.9657 - recall: 0.9651 - f1: 0.9654 - acc: 0.9651 - val_loss: 2.1459 - val_precision: 0.6832 - val_recall: 0.6797 - val_f1: 0.6815 - val_acc: 0.6849\n","\n","Epoch 00031: val_loss did not improve from 0.16083\n","Epoch 32/100\n"," - 49s - loss: 0.1133 - precision: 0.9578 - recall: 0.9549 - f1: 0.9563 - acc: 0.9561 - val_loss: 1.8879 - val_precision: 0.6763 - val_recall: 0.6745 - val_f1: 0.6754 - val_acc: 0.6745\n","\n","Epoch 00032: val_loss did not improve from 0.16083\n","Epoch 33/100\n"," - 49s - loss: 0.0709 - precision: 0.9747 - recall: 0.9736 - f1: 0.9741 - acc: 0.9748 - val_loss: 0.2972 - val_precision: 0.9110 - val_recall: 0.9062 - val_f1: 0.9086 - val_acc: 0.9062\n","\n","Epoch 00033: val_loss did not improve from 0.16083\n","Epoch 34/100\n"," - 49s - loss: 0.1064 - precision: 0.9637 - recall: 0.9585 - f1: 0.9611 - acc: 0.9615 - val_loss: 1.9464 - val_precision: 0.6613 - val_recall: 0.6510 - val_f1: 0.6561 - val_acc: 0.6562\n","\n","Epoch 00034: val_loss did not improve from 0.16083\n","Epoch 35/100\n"," - 49s - loss: 0.0813 - precision: 0.9735 - recall: 0.9718 - f1: 0.9726 - acc: 0.9724 - val_loss: 1.0404 - val_precision: 0.7482 - val_recall: 0.7422 - val_f1: 0.7451 - val_acc: 0.7448\n","\n","Epoch 00035: val_loss did not improve from 0.16083\n","Epoch 36/100\n"," - 49s - loss: 0.0746 - precision: 0.9747 - recall: 0.9736 - f1: 0.9741 - acc: 0.9742 - val_loss: 1.4606 - val_precision: 0.8125 - val_recall: 0.8125 - val_f1: 0.8125 - val_acc: 0.8125\n","\n","Epoch 00036: val_loss did not improve from 0.16083\n","Epoch 37/100\n"," - 49s - loss: 0.1201 - precision: 0.9548 - recall: 0.9525 - f1: 0.9537 - acc: 0.9531 - val_loss: 1.0845 - val_precision: 0.7639 - val_recall: 0.7500 - val_f1: 0.7568 - val_acc: 0.7630\n","\n","Epoch 00037: val_loss did not improve from 0.16083\n","Epoch 38/100\n"," - 49s - loss: 0.0844 - precision: 0.9704 - recall: 0.9663 - f1: 0.9684 - acc: 0.9675 - val_loss: 0.1239 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n","\n","Epoch 00038: val_loss improved from 0.16083 to 0.12386, saving model to BRAIN_TUMOR_FOLD_1.h5\n","Epoch 39/100\n"," - 49s - loss: 0.0656 - precision: 0.9759 - recall: 0.9736 - f1: 0.9747 - acc: 0.9742 - val_loss: 2.3069 - val_precision: 0.5748 - val_recall: 0.5703 - val_f1: 0.5725 - val_acc: 0.5729\n","\n","Epoch 00039: val_loss did not improve from 0.12386\n","Epoch 40/100\n"," - 49s - loss: 0.1260 - precision: 0.9596 - recall: 0.9555 - f1: 0.9575 - acc: 0.9585 - val_loss: 1.8411 - val_precision: 0.6458 - val_recall: 0.6458 - val_f1: 0.6458 - val_acc: 0.6458\n","\n","Epoch 00040: val_loss did not improve from 0.12386\n","Epoch 41/100\n"," - 49s - loss: 0.0660 - precision: 0.9783 - recall: 0.9760 - f1: 0.9771 - acc: 0.9760 - val_loss: 1.1322 - val_precision: 0.7400 - val_recall: 0.7344 - val_f1: 0.7372 - val_acc: 0.7396\n","\n","Epoch 00041: val_loss did not improve from 0.12386\n","Epoch 42/100\n"," - 50s - loss: 0.0585 - precision: 0.9777 - recall: 0.9754 - f1: 0.9765 - acc: 0.9766 - val_loss: 2.9070 - val_precision: 0.6307 - val_recall: 0.6276 - val_f1: 0.6292 - val_acc: 0.6328\n","\n","Epoch 00042: val_loss did not improve from 0.12386\n","Epoch 43/100\n"," - 49s - loss: 0.0837 - precision: 0.9705 - recall: 0.9700 - f1: 0.9702 - acc: 0.9706 - val_loss: 0.1879 - val_precision: 0.9245 - val_recall: 0.9245 - val_f1: 0.9245 - val_acc: 0.9245\n","\n","Epoch 00043: val_loss did not improve from 0.12386\n","Epoch 44/100\n"," - 49s - loss: 0.0642 - precision: 0.9790 - recall: 0.9778 - f1: 0.9784 - acc: 0.9784 - val_loss: 0.5918 - val_precision: 0.8411 - val_recall: 0.8411 - val_f1: 0.8411 - val_acc: 0.8411\n","\n","Epoch 00044: val_loss did not improve from 0.12386\n","Epoch 45/100\n"," - 49s - loss: 0.0853 - precision: 0.9717 - recall: 0.9700 - f1: 0.9708 - acc: 0.9706 - val_loss: 0.1065 - val_precision: 0.9688 - val_recall: 0.9688 - val_f1: 0.9687 - val_acc: 0.9688\n","\n","Epoch 00045: val_loss improved from 0.12386 to 0.10650, saving model to BRAIN_TUMOR_FOLD_1.h5\n","Epoch 46/100\n"," - 49s - loss: 0.0636 - precision: 0.9789 - recall: 0.9772 - f1: 0.9780 - acc: 0.9778 - val_loss: 0.1030 - val_precision: 0.9660 - val_recall: 0.9635 - val_f1: 0.9648 - val_acc: 0.9635\n","\n","Epoch 00046: val_loss improved from 0.10650 to 0.10300, saving model to BRAIN_TUMOR_FOLD_1.h5\n","Epoch 47/100\n"," - 49s - loss: 0.0551 - precision: 0.9814 - recall: 0.9802 - f1: 0.9808 - acc: 0.9808 - val_loss: 1.8559 - val_precision: 0.6661 - val_recall: 0.6458 - val_f1: 0.6557 - val_acc: 0.6641\n","\n","Epoch 00047: val_loss did not improve from 0.10300\n","Epoch 48/100\n"," - 49s - loss: 0.0791 - precision: 0.9747 - recall: 0.9736 - f1: 0.9741 - acc: 0.9742 - val_loss: 0.2303 - val_precision: 0.9476 - val_recall: 0.9401 - val_f1: 0.9438 - val_acc: 0.9479\n","\n","Epoch 00048: val_loss did not improve from 0.10300\n","Epoch 49/100\n"," - 49s - loss: 0.0802 - precision: 0.9741 - recall: 0.9730 - f1: 0.9735 - acc: 0.9736 - val_loss: 3.4877 - val_precision: 0.5651 - val_recall: 0.5651 - val_f1: 0.5651 - val_acc: 0.5651\n","\n","Epoch 00049: val_loss did not improve from 0.10300\n","Epoch 50/100\n"," - 49s - loss: 0.0553 - precision: 0.9807 - recall: 0.9796 - f1: 0.9801 - acc: 0.9808 - val_loss: 3.2987 - val_precision: 0.5573 - val_recall: 0.5573 - val_f1: 0.5573 - val_acc: 0.5573\n","\n","Epoch 00050: val_loss did not improve from 0.10300\n","Epoch 51/100\n"," - 49s - loss: 0.0606 - precision: 0.9819 - recall: 0.9790 - f1: 0.9804 - acc: 0.9802 - val_loss: 0.9288 - val_precision: 0.8411 - val_recall: 0.8411 - val_f1: 0.8411 - val_acc: 0.8411\n","\n","Epoch 00051: val_loss did not improve from 0.10300\n","Epoch 52/100\n"," - 49s - loss: 0.0672 - precision: 0.9771 - recall: 0.9760 - f1: 0.9765 - acc: 0.9772 - val_loss: 3.4902 - val_precision: 0.4818 - val_recall: 0.4818 - val_f1: 0.4818 - val_acc: 0.4818\n","\n","Epoch 00052: val_loss did not improve from 0.10300\n","Epoch 53/100\n"," - 49s - loss: 0.0574 - precision: 0.9783 - recall: 0.9772 - f1: 0.9777 - acc: 0.9778 - val_loss: 0.7952 - val_precision: 0.7821 - val_recall: 0.7760 - val_f1: 0.7790 - val_acc: 0.7812\n","\n","Epoch 00053: val_loss did not improve from 0.10300\n","Epoch 54/100\n"," - 49s - loss: 0.0478 - precision: 0.9825 - recall: 0.9808 - f1: 0.9816 - acc: 0.9820 - val_loss: 0.1476 - val_precision: 0.9323 - val_recall: 0.9323 - val_f1: 0.9323 - val_acc: 0.9323\n","\n","Epoch 00054: val_loss did not improve from 0.10300\n","Epoch 55/100\n"," - 49s - loss: 0.0434 - precision: 0.9856 - recall: 0.9856 - f1: 0.9856 - acc: 0.9856 - val_loss: 0.9842 - val_precision: 0.7848 - val_recall: 0.7578 - val_f1: 0.7710 - val_acc: 0.7578\n","\n","Epoch 00055: val_loss did not improve from 0.10300\n","Epoch 56/100\n"," - 49s - loss: 0.0833 - precision: 0.9705 - recall: 0.9694 - f1: 0.9699 - acc: 0.9706 - val_loss: 6.5928 - val_precision: 0.4974 - val_recall: 0.4974 - val_f1: 0.4974 - val_acc: 0.4974\n","\n","Epoch 00056: val_loss did not improve from 0.10300\n","Epoch 57/100\n"," - 49s - loss: 0.1118 - precision: 0.9620 - recall: 0.9603 - f1: 0.9612 - acc: 0.9609 - val_loss: 4.2476 - val_precision: 0.3750 - val_recall: 0.3750 - val_f1: 0.3750 - val_acc: 0.3750\n","\n","Epoch 00057: val_loss did not improve from 0.10300\n","Epoch 58/100\n"," - 49s - loss: 0.0728 - precision: 0.9717 - recall: 0.9700 - f1: 0.9708 - acc: 0.9712 - val_loss: 1.6726 - val_precision: 0.6276 - val_recall: 0.6224 - val_f1: 0.6250 - val_acc: 0.6224\n","\n","Epoch 00058: val_loss did not improve from 0.10300\n","Epoch 59/100\n"," - 50s - loss: 0.0454 - precision: 0.9850 - recall: 0.9838 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.4066 - val_precision: 0.8746 - val_recall: 0.8724 - val_f1: 0.8735 - val_acc: 0.8724\n","\n","Epoch 00059: val_loss did not improve from 0.10300\n","Epoch 60/100\n"," - 49s - loss: 0.0679 - precision: 0.9783 - recall: 0.9772 - f1: 0.9777 - acc: 0.9772 - val_loss: 6.0102 - val_precision: 0.4583 - val_recall: 0.4583 - val_f1: 0.4583 - val_acc: 0.4583\n","\n","Epoch 00060: val_loss did not improve from 0.10300\n","Epoch 61/100\n"," - 49s - loss: 0.0754 - precision: 0.9747 - recall: 0.9712 - f1: 0.9729 - acc: 0.9736 - val_loss: 0.7214 - val_precision: 0.8269 - val_recall: 0.8203 - val_f1: 0.8235 - val_acc: 0.8229\n","\n","Epoch 00061: val_loss did not improve from 0.10300\n","Epoch 62/100\n"," - 49s - loss: 0.0600 - precision: 0.9789 - recall: 0.9766 - f1: 0.9777 - acc: 0.9778 - val_loss: 0.6217 - val_precision: 0.8672 - val_recall: 0.8490 - val_f1: 0.8580 - val_acc: 0.8594\n","\n","Epoch 00062: val_loss did not improve from 0.10300\n","Epoch 63/100\n"," - 49s - loss: 0.0413 - precision: 0.9856 - recall: 0.9856 - f1: 0.9856 - acc: 0.9856 - val_loss: 1.6126 - val_precision: 0.6953 - val_recall: 0.6953 - val_f1: 0.6953 - val_acc: 0.6953\n","\n","Epoch 00063: val_loss did not improve from 0.10300\n","Epoch 64/100\n"," - 49s - loss: 0.0771 - precision: 0.9735 - recall: 0.9724 - f1: 0.9729 - acc: 0.9730 - val_loss: 4.9413 - val_precision: 0.5495 - val_recall: 0.5495 - val_f1: 0.5495 - val_acc: 0.5495\n","\n","Epoch 00064: val_loss did not improve from 0.10300\n","Epoch 65/100\n"," - 49s - loss: 0.0906 - precision: 0.9711 - recall: 0.9688 - f1: 0.9699 - acc: 0.9706 - val_loss: 0.6611 - val_precision: 0.8687 - val_recall: 0.8594 - val_f1: 0.8640 - val_acc: 0.8672\n","\n","Epoch 00065: val_loss did not improve from 0.10300\n","Epoch 66/100\n"," - 49s - loss: 0.0545 - precision: 0.9819 - recall: 0.9808 - f1: 0.9813 - acc: 0.9814 - val_loss: 1.0198 - val_precision: 0.7988 - val_recall: 0.7943 - val_f1: 0.7965 - val_acc: 0.7943\n","\n","Epoch 00066: val_loss did not improve from 0.10300\n","Epoch 67/100\n"," - 49s - loss: 0.0486 - precision: 0.9820 - recall: 0.9808 - f1: 0.9814 - acc: 0.9808 - val_loss: 0.1700 - val_precision: 0.9632 - val_recall: 0.9479 - val_f1: 0.9554 - val_acc: 0.9609\n","\n","Epoch 00067: val_loss did not improve from 0.10300\n","Epoch 68/100\n"," - 49s - loss: 0.1058 - precision: 0.9602 - recall: 0.9585 - f1: 0.9594 - acc: 0.9591 - val_loss: 0.1324 - val_precision: 0.9297 - val_recall: 0.9297 - val_f1: 0.9297 - val_acc: 0.9297\n","\n","Epoch 00068: val_loss did not improve from 0.10300\n","Epoch 69/100\n"," - 49s - loss: 0.1044 - precision: 0.9625 - recall: 0.9585 - f1: 0.9605 - acc: 0.9597 - val_loss: 0.9353 - val_precision: 0.8066 - val_recall: 0.7917 - val_f1: 0.7990 - val_acc: 0.8047\n","\n","Epoch 00069: val_loss did not improve from 0.10300\n","Epoch 70/100\n"," - 49s - loss: 0.0716 - precision: 0.9735 - recall: 0.9712 - f1: 0.9723 - acc: 0.9718 - val_loss: 0.5319 - val_precision: 0.8205 - val_recall: 0.8099 - val_f1: 0.8151 - val_acc: 0.8229\n","\n","Epoch 00070: val_loss did not improve from 0.10300\n","Epoch 71/100\n"," - 49s - loss: 0.0724 - precision: 0.9735 - recall: 0.9718 - f1: 0.9726 - acc: 0.9718 - val_loss: 0.2733 - val_precision: 0.9139 - val_recall: 0.9115 - val_f1: 0.9126 - val_acc: 0.9115\n","\n","Epoch 00071: val_loss did not improve from 0.10300\n","Epoch 72/100\n"," - 49s - loss: 0.0534 - precision: 0.9807 - recall: 0.9784 - f1: 0.9795 - acc: 0.9796 - val_loss: 0.6870 - val_precision: 0.8672 - val_recall: 0.8672 - val_f1: 0.8672 - val_acc: 0.8672\n","\n","Epoch 00072: val_loss did not improve from 0.10300\n","Epoch 73/100\n"," - 49s - loss: 0.0650 - precision: 0.9789 - recall: 0.9760 - f1: 0.9774 - acc: 0.9778 - val_loss: 0.1815 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n","\n","Epoch 00073: val_loss did not improve from 0.10300\n","Epoch 74/100\n"," - 49s - loss: 0.0392 - precision: 0.9838 - recall: 0.9832 - f1: 0.9835 - acc: 0.9838 - val_loss: 0.1303 - val_precision: 0.9659 - val_recall: 0.9609 - val_f1: 0.9634 - val_acc: 0.9661\n","\n","Epoch 00074: val_loss did not improve from 0.10300\n","Epoch 75/100\n"," - 49s - loss: 0.0458 - precision: 0.9802 - recall: 0.9802 - f1: 0.9802 - acc: 0.9802 - val_loss: 0.6508 - val_precision: 0.7863 - val_recall: 0.7760 - val_f1: 0.7811 - val_acc: 0.7839\n","\n","Epoch 00075: val_loss did not improve from 0.10300\n","Epoch 76/100\n"," - 49s - loss: 0.0355 - precision: 0.9886 - recall: 0.9880 - f1: 0.9883 - acc: 0.9880 - val_loss: 0.3689 - val_precision: 0.9036 - val_recall: 0.9036 - val_f1: 0.9036 - val_acc: 0.9036\n","\n","Epoch 00076: val_loss did not improve from 0.10300\n","Epoch 77/100\n"," - 49s - loss: 0.0625 - precision: 0.9759 - recall: 0.9742 - f1: 0.9750 - acc: 0.9760 - val_loss: 1.4709 - val_precision: 0.7070 - val_recall: 0.6797 - val_f1: 0.6930 - val_acc: 0.6875\n","\n","Epoch 00077: val_loss did not improve from 0.10300\n","Epoch 78/100\n"," - 49s - loss: 0.1038 - precision: 0.9633 - recall: 0.9627 - f1: 0.9630 - acc: 0.9627 - val_loss: 0.3281 - val_precision: 0.8927 - val_recall: 0.8880 - val_f1: 0.8903 - val_acc: 0.8880\n","\n","Epoch 00078: val_loss did not improve from 0.10300\n","Epoch 79/100\n"," - 50s - loss: 0.0714 - precision: 0.9789 - recall: 0.9784 - f1: 0.9786 - acc: 0.9790 - val_loss: 2.9608 - val_precision: 0.6156 - val_recall: 0.6094 - val_f1: 0.6124 - val_acc: 0.6094\n","\n","Epoch 00079: val_loss did not improve from 0.10300\n","Epoch 80/100\n"," - 50s - loss: 0.0455 - precision: 0.9850 - recall: 0.9844 - f1: 0.9847 - acc: 0.9844 - val_loss: 0.6008 - val_precision: 0.8461 - val_recall: 0.8438 - val_f1: 0.8449 - val_acc: 0.8464\n","\n","Epoch 00080: val_loss did not improve from 0.10300\n","Epoch 81/100\n"," - 49s - loss: 0.0537 - precision: 0.9820 - recall: 0.9808 - f1: 0.9814 - acc: 0.9808 - val_loss: 0.6910 - val_precision: 0.8089 - val_recall: 0.8047 - val_f1: 0.8068 - val_acc: 0.8073\n","\n","Epoch 00081: val_loss did not improve from 0.10300\n","Epoch 82/100\n"," - 49s - loss: 0.0597 - precision: 0.9808 - recall: 0.9796 - f1: 0.9802 - acc: 0.9802 - val_loss: 1.4244 - val_precision: 0.7986 - val_recall: 0.7943 - val_f1: 0.7964 - val_acc: 0.7969\n","\n","Epoch 00082: val_loss did not improve from 0.10300\n","Epoch 83/100\n"," - 49s - loss: 0.0540 - precision: 0.9819 - recall: 0.9790 - f1: 0.9804 - acc: 0.9820 - val_loss: 3.7945 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n","\n","Epoch 00083: val_loss did not improve from 0.10300\n","Epoch 84/100\n"," - 49s - loss: 0.0749 - precision: 0.9741 - recall: 0.9730 - f1: 0.9735 - acc: 0.9730 - val_loss: 1.4947 - val_precision: 0.7370 - val_recall: 0.7370 - val_f1: 0.7370 - val_acc: 0.7370\n","\n","Epoch 00084: val_loss did not improve from 0.10300\n","Epoch 85/100\n"," - 49s - loss: 0.0501 - precision: 0.9820 - recall: 0.9820 - f1: 0.9820 - acc: 0.9820 - val_loss: 0.4694 - val_precision: 0.8851 - val_recall: 0.8828 - val_f1: 0.8839 - val_acc: 0.8854\n","\n","Epoch 00085: val_loss did not improve from 0.10300\n","Epoch 86/100\n"," - 49s - loss: 0.0395 - precision: 0.9844 - recall: 0.9838 - f1: 0.9841 - acc: 0.9838 - val_loss: 0.2733 - val_precision: 0.9323 - val_recall: 0.9323 - val_f1: 0.9323 - val_acc: 0.9323\n","\n","Epoch 00086: val_loss did not improve from 0.10300\n","Epoch 87/100\n"," - 49s - loss: 0.0552 - precision: 0.9783 - recall: 0.9778 - f1: 0.9781 - acc: 0.9784 - val_loss: 2.3628 - val_precision: 0.5990 - val_recall: 0.5990 - val_f1: 0.5990 - val_acc: 0.5990\n","\n","Epoch 00087: val_loss did not improve from 0.10300\n","Epoch 88/100\n"," - 50s - loss: 0.0433 - precision: 0.9856 - recall: 0.9844 - f1: 0.9850 - acc: 0.9844 - val_loss: 7.9648 - val_precision: 0.4531 - val_recall: 0.4531 - val_f1: 0.4531 - val_acc: 0.4531\n","\n","Epoch 00088: val_loss did not improve from 0.10300\n","Epoch 89/100\n"," - 49s - loss: 0.0585 - precision: 0.9783 - recall: 0.9772 - f1: 0.9777 - acc: 0.9778 - val_loss: 0.2828 - val_precision: 0.9062 - val_recall: 0.9062 - val_f1: 0.9062 - val_acc: 0.9062\n","\n","Epoch 00089: val_loss did not improve from 0.10300\n","Epoch 90/100\n"," - 50s - loss: 0.0586 - precision: 0.9760 - recall: 0.9760 - f1: 0.9760 - acc: 0.9760 - val_loss: 3.8684 - val_precision: 0.5026 - val_recall: 0.5026 - val_f1: 0.5026 - val_acc: 0.5026\n","\n","Epoch 00090: val_loss did not improve from 0.10300\n","Epoch 91/100\n"," - 49s - loss: 0.0569 - precision: 0.9831 - recall: 0.9814 - f1: 0.9823 - acc: 0.9820 - val_loss: 0.6643 - val_precision: 0.8203 - val_recall: 0.8203 - val_f1: 0.8203 - val_acc: 0.8203\n","\n","Epoch 00091: val_loss did not improve from 0.10300\n","Epoch 92/100\n"," - 49s - loss: 0.0440 - precision: 0.9820 - recall: 0.9814 - f1: 0.9817 - acc: 0.9820 - val_loss: 0.3035 - val_precision: 0.9245 - val_recall: 0.9245 - val_f1: 0.9245 - val_acc: 0.9245\n","\n","Epoch 00092: val_loss did not improve from 0.10300\n","Epoch 93/100\n"," - 50s - loss: 0.0242 - precision: 0.9910 - recall: 0.9910 - f1: 0.9910 - acc: 0.9910 - val_loss: 0.1940 - val_precision: 0.9479 - val_recall: 0.9479 - val_f1: 0.9479 - val_acc: 0.9479\n","\n","Epoch 00093: val_loss did not improve from 0.10300\n","Epoch 94/100\n"," - 49s - loss: 0.0375 - precision: 0.9886 - recall: 0.9874 - f1: 0.9880 - acc: 0.9880 - val_loss: 3.4068 - val_precision: 0.5495 - val_recall: 0.5495 - val_f1: 0.5495 - val_acc: 0.5495\n","\n","Epoch 00094: val_loss did not improve from 0.10300\n","Epoch 95/100\n"," - 49s - loss: 0.0668 - precision: 0.9784 - recall: 0.9772 - f1: 0.9778 - acc: 0.9772 - val_loss: 0.9907 - val_precision: 0.8516 - val_recall: 0.8516 - val_f1: 0.8516 - val_acc: 0.8516\n","\n","Epoch 00095: val_loss did not improve from 0.10300\n","Epoch 96/100\n"," - 50s - loss: 0.0514 - precision: 0.9838 - recall: 0.9832 - f1: 0.9835 - acc: 0.9838 - val_loss: 0.6983 - val_precision: 0.8068 - val_recall: 0.8047 - val_f1: 0.8058 - val_acc: 0.8047\n","\n","Epoch 00096: val_loss did not improve from 0.10300\n","Epoch 97/100\n"," - 49s - loss: 0.0611 - precision: 0.9784 - recall: 0.9778 - f1: 0.9781 - acc: 0.9784 - val_loss: 0.6470 - val_precision: 0.8378 - val_recall: 0.8333 - val_f1: 0.8355 - val_acc: 0.8333\n","\n","Epoch 00097: val_loss did not improve from 0.10300\n","Epoch 98/100\n"," - 49s - loss: 0.0603 - precision: 0.9783 - recall: 0.9772 - f1: 0.9777 - acc: 0.9778 - val_loss: 0.1445 - val_precision: 0.9635 - val_recall: 0.9635 - val_f1: 0.9635 - val_acc: 0.9635\n","\n","Epoch 00098: val_loss did not improve from 0.10300\n","Epoch 99/100\n"," - 50s - loss: 0.0480 - precision: 0.9862 - recall: 0.9856 - f1: 0.9859 - acc: 0.9862 - val_loss: 0.1159 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n","\n","Epoch 00099: val_loss did not improve from 0.10300\n","Epoch 100/100\n"," - 49s - loss: 0.0376 - precision: 0.9892 - recall: 0.9886 - f1: 0.9889 - acc: 0.9886 - val_loss: 0.2638 - val_precision: 0.9323 - val_recall: 0.9323 - val_f1: 0.9323 - val_acc: 0.9323\n","\n","Epoch 00100: val_loss did not improve from 0.10300\n","Epoch 1/100\n"," - 52s - loss: 0.1591 - precision: 0.9479 - recall: 0.9411 - f1: 0.9445 - acc: 0.9435 - val_loss: 7.8935 - val_precision: 0.5078 - val_recall: 0.5078 - val_f1: 0.5078 - val_acc: 0.5078\n","\n","Epoch 00001: val_loss improved from inf to 7.89346, saving model to BRAIN_TUMOR_FOLD_2.h5\n","Epoch 2/100\n"," - 48s - loss: 0.1418 - precision: 0.9450 - recall: 0.9405 - f1: 0.9427 - acc: 0.9435 - val_loss: 1.3557 - val_precision: 0.6300 - val_recall: 0.6250 - val_f1: 0.6275 - val_acc: 0.6250\n","\n","Epoch 00002: val_loss improved from 7.89346 to 1.35573, saving model to BRAIN_TUMOR_FOLD_2.h5\n","Epoch 3/100\n"," - 50s - loss: 0.1375 - precision: 0.9506 - recall: 0.9489 - f1: 0.9498 - acc: 0.9495 - val_loss: 4.1938 - val_precision: 0.4766 - val_recall: 0.4766 - val_f1: 0.4766 - val_acc: 0.4766\n","\n","Epoch 00003: val_loss did not improve from 1.35573\n","Epoch 4/100\n"," - 50s - loss: 0.1115 - precision: 0.9618 - recall: 0.9579 - f1: 0.9599 - acc: 0.9591 - val_loss: 0.5982 - val_precision: 0.8178 - val_recall: 0.7943 - val_f1: 0.8058 - val_acc: 0.8177\n","\n","Epoch 00004: val_loss improved from 1.35573 to 0.59818, saving model to BRAIN_TUMOR_FOLD_2.h5\n","Epoch 5/100\n"," - 50s - loss: 0.0898 - precision: 0.9645 - recall: 0.9633 - f1: 0.9639 - acc: 0.9639 - val_loss: 6.5039 - val_precision: 0.3420 - val_recall: 0.3411 - val_f1: 0.3416 - val_acc: 0.3411\n","\n","Epoch 00005: val_loss did not improve from 0.59818\n","Epoch 6/100\n"," - 50s - loss: 0.1044 - precision: 0.9638 - recall: 0.9603 - f1: 0.9620 - acc: 0.9615 - val_loss: 0.3091 - val_precision: 0.9103 - val_recall: 0.8958 - val_f1: 0.9030 - val_acc: 0.9010\n","\n","Epoch 00006: val_loss improved from 0.59818 to 0.30914, saving model to BRAIN_TUMOR_FOLD_2.h5\n","Epoch 7/100\n"," - 49s - loss: 0.1377 - precision: 0.9535 - recall: 0.9501 - f1: 0.9518 - acc: 0.9525 - val_loss: 0.8129 - val_precision: 0.7885 - val_recall: 0.7865 - val_f1: 0.7875 - val_acc: 0.7865\n","\n","Epoch 00007: val_loss did not improve from 0.30914\n","Epoch 8/100\n"," - 49s - loss: 0.0974 - precision: 0.9651 - recall: 0.9633 - f1: 0.9642 - acc: 0.9645 - val_loss: 0.5324 - val_precision: 0.7880 - val_recall: 0.7839 - val_f1: 0.7859 - val_acc: 0.7891\n","\n","Epoch 00008: val_loss did not improve from 0.30914\n","Epoch 9/100\n"," - 50s - loss: 0.0981 - precision: 0.9699 - recall: 0.9675 - f1: 0.9687 - acc: 0.9688 - val_loss: 0.6626 - val_precision: 0.8325 - val_recall: 0.8281 - val_f1: 0.8303 - val_acc: 0.8333\n","\n","Epoch 00009: val_loss did not improve from 0.30914\n","Epoch 10/100\n"," - 50s - loss: 0.0930 - precision: 0.9705 - recall: 0.9675 - f1: 0.9690 - acc: 0.9681 - val_loss: 5.5629 - val_precision: 0.4284 - val_recall: 0.4271 - val_f1: 0.4278 - val_acc: 0.4271\n","\n","Epoch 00010: val_loss did not improve from 0.30914\n","Epoch 11/100\n"," - 50s - loss: 0.0856 - precision: 0.9729 - recall: 0.9700 - f1: 0.9714 - acc: 0.9706 - val_loss: 5.2385 - val_precision: 0.4349 - val_recall: 0.4349 - val_f1: 0.4349 - val_acc: 0.4349\n","\n","Epoch 00011: val_loss did not improve from 0.30914\n","Epoch 12/100\n"," - 49s - loss: 0.0994 - precision: 0.9633 - recall: 0.9615 - f1: 0.9624 - acc: 0.9615 - val_loss: 1.4125 - val_precision: 0.7394 - val_recall: 0.7318 - val_f1: 0.7355 - val_acc: 0.7344\n","\n","Epoch 00012: val_loss did not improve from 0.30914\n","Epoch 13/100\n"," - 49s - loss: 0.0668 - precision: 0.9790 - recall: 0.9790 - f1: 0.9790 - acc: 0.9790 - val_loss: 8.7624 - val_precision: 0.3203 - val_recall: 0.3203 - val_f1: 0.3203 - val_acc: 0.3203\n","\n","Epoch 00013: val_loss did not improve from 0.30914\n","Epoch 14/100\n"," - 50s - loss: 0.0801 - precision: 0.9729 - recall: 0.9700 - f1: 0.9714 - acc: 0.9718 - val_loss: 0.1956 - val_precision: 0.9427 - val_recall: 0.9427 - val_f1: 0.9427 - val_acc: 0.9427\n","\n","Epoch 00014: val_loss improved from 0.30914 to 0.19562, saving model to BRAIN_TUMOR_FOLD_2.h5\n","Epoch 15/100\n"," - 49s - loss: 0.0629 - precision: 0.9771 - recall: 0.9760 - f1: 0.9765 - acc: 0.9760 - val_loss: 4.9200 - val_precision: 0.5026 - val_recall: 0.5026 - val_f1: 0.5026 - val_acc: 0.5026\n","\n","Epoch 00015: val_loss did not improve from 0.19562\n","Epoch 16/100\n"," - 50s - loss: 0.1107 - precision: 0.9650 - recall: 0.9627 - f1: 0.9639 - acc: 0.9645 - val_loss: 1.9395 - val_precision: 0.6231 - val_recall: 0.6198 - val_f1: 0.6214 - val_acc: 0.6198\n","\n","Epoch 00016: val_loss did not improve from 0.19562\n","Epoch 17/100\n"," - 50s - loss: 0.0661 - precision: 0.9760 - recall: 0.9760 - f1: 0.9760 - acc: 0.9760 - val_loss: 1.1572 - val_precision: 0.7129 - val_recall: 0.7057 - val_f1: 0.7092 - val_acc: 0.7135\n","\n","Epoch 00017: val_loss did not improve from 0.19562\n","Epoch 18/100\n"," - 50s - loss: 0.0875 - precision: 0.9681 - recall: 0.9675 - f1: 0.9678 - acc: 0.9681 - val_loss: 2.0215 - val_precision: 0.7258 - val_recall: 0.7240 - val_f1: 0.7249 - val_acc: 0.7240\n","\n","Epoch 00018: val_loss did not improve from 0.19562\n","Epoch 19/100\n"," - 49s - loss: 0.0897 - precision: 0.9711 - recall: 0.9694 - f1: 0.9702 - acc: 0.9700 - val_loss: 7.0563 - val_precision: 0.4323 - val_recall: 0.4323 - val_f1: 0.4323 - val_acc: 0.4323\n","\n","Epoch 00019: val_loss did not improve from 0.19562\n","Epoch 20/100\n"," - 49s - loss: 0.0742 - precision: 0.9717 - recall: 0.9712 - f1: 0.9714 - acc: 0.9712 - val_loss: 6.0892 - val_precision: 0.4349 - val_recall: 0.4349 - val_f1: 0.4349 - val_acc: 0.4349\n","\n","Epoch 00020: val_loss did not improve from 0.19562\n","Epoch 21/100\n"," - 50s - loss: 0.1084 - precision: 0.9627 - recall: 0.9609 - f1: 0.9618 - acc: 0.9621 - val_loss: 3.3109 - val_precision: 0.4766 - val_recall: 0.4766 - val_f1: 0.4766 - val_acc: 0.4766\n","\n","Epoch 00021: val_loss did not improve from 0.19562\n","Epoch 22/100\n"," - 49s - loss: 0.0770 - precision: 0.9729 - recall: 0.9724 - f1: 0.9726 - acc: 0.9724 - val_loss: 1.0480 - val_precision: 0.7546 - val_recall: 0.7526 - val_f1: 0.7536 - val_acc: 0.7552\n","\n","Epoch 00022: val_loss did not improve from 0.19562\n","Epoch 23/100\n"," - 50s - loss: 0.0681 - precision: 0.9747 - recall: 0.9736 - f1: 0.9741 - acc: 0.9748 - val_loss: 2.7497 - val_precision: 0.6450 - val_recall: 0.6432 - val_f1: 0.6441 - val_acc: 0.6432\n","\n","Epoch 00023: val_loss did not improve from 0.19562\n","Epoch 24/100\n"," - 50s - loss: 0.0582 - precision: 0.9789 - recall: 0.9766 - f1: 0.9777 - acc: 0.9778 - val_loss: 5.2964 - val_precision: 0.5312 - val_recall: 0.5312 - val_f1: 0.5312 - val_acc: 0.5312\n","\n","Epoch 00024: val_loss did not improve from 0.19562\n","Epoch 25/100\n"," - 49s - loss: 0.0667 - precision: 0.9748 - recall: 0.9748 - f1: 0.9748 - acc: 0.9748 - val_loss: 0.9745 - val_precision: 0.7710 - val_recall: 0.7630 - val_f1: 0.7670 - val_acc: 0.7656\n","\n","Epoch 00025: val_loss did not improve from 0.19562\n","Epoch 26/100\n"," - 50s - loss: 0.0820 - precision: 0.9687 - recall: 0.9675 - f1: 0.9681 - acc: 0.9681 - val_loss: 3.8784 - val_precision: 0.5260 - val_recall: 0.5260 - val_f1: 0.5260 - val_acc: 0.5260\n","\n","Epoch 00026: val_loss did not improve from 0.19562\n","Epoch 27/100\n"," - 49s - loss: 0.0727 - precision: 0.9790 - recall: 0.9784 - f1: 0.9787 - acc: 0.9790 - val_loss: 3.3100 - val_precision: 0.4896 - val_recall: 0.4896 - val_f1: 0.4896 - val_acc: 0.4896\n","\n","Epoch 00027: val_loss did not improve from 0.19562\n","Epoch 28/100\n"," - 49s - loss: 0.0442 - precision: 0.9868 - recall: 0.9850 - f1: 0.9859 - acc: 0.9862 - val_loss: 3.7464 - val_precision: 0.4766 - val_recall: 0.4766 - val_f1: 0.4766 - val_acc: 0.4766\n","\n","Epoch 00028: val_loss did not improve from 0.19562\n","Epoch 29/100\n"," - 50s - loss: 0.0733 - precision: 0.9687 - recall: 0.9681 - f1: 0.9684 - acc: 0.9681 - val_loss: 4.5631 - val_precision: 0.5755 - val_recall: 0.5755 - val_f1: 0.5755 - val_acc: 0.5755\n","\n","Epoch 00029: val_loss did not improve from 0.19562\n","Epoch 30/100\n"," - 50s - loss: 0.0569 - precision: 0.9832 - recall: 0.9832 - f1: 0.9832 - acc: 0.9832 - val_loss: 0.1600 - val_precision: 0.9426 - val_recall: 0.9401 - val_f1: 0.9413 - val_acc: 0.9401\n","\n","Epoch 00030: val_loss improved from 0.19562 to 0.16001, saving model to BRAIN_TUMOR_FOLD_2.h5\n","Epoch 31/100\n"," - 50s - loss: 0.0508 - precision: 0.9778 - recall: 0.9772 - f1: 0.9775 - acc: 0.9778 - val_loss: 0.5063 - val_precision: 0.8490 - val_recall: 0.8490 - val_f1: 0.8490 - val_acc: 0.8490\n","\n","Epoch 00031: val_loss did not improve from 0.16001\n","Epoch 32/100\n"," - 50s - loss: 0.0689 - precision: 0.9789 - recall: 0.9778 - f1: 0.9784 - acc: 0.9784 - val_loss: 0.4871 - val_precision: 0.8177 - val_recall: 0.8177 - val_f1: 0.8177 - val_acc: 0.8177\n","\n","Epoch 00032: val_loss did not improve from 0.16001\n","Epoch 33/100\n"," - 49s - loss: 0.0512 - precision: 0.9820 - recall: 0.9814 - f1: 0.9817 - acc: 0.9814 - val_loss: 0.5124 - val_precision: 0.8122 - val_recall: 0.8099 - val_f1: 0.8110 - val_acc: 0.8099\n","\n","Epoch 00033: val_loss did not improve from 0.16001\n","Epoch 34/100\n"," - 49s - loss: 0.0781 - precision: 0.9693 - recall: 0.9675 - f1: 0.9684 - acc: 0.9681 - val_loss: 0.9279 - val_precision: 0.7883 - val_recall: 0.7865 - val_f1: 0.7874 - val_acc: 0.7865\n","\n","Epoch 00034: val_loss did not improve from 0.16001\n","Epoch 35/100\n"," - 49s - loss: 0.1064 - precision: 0.9560 - recall: 0.9543 - f1: 0.9552 - acc: 0.9549 - val_loss: 4.0907 - val_precision: 0.5391 - val_recall: 0.5391 - val_f1: 0.5391 - val_acc: 0.5391\n","\n","Epoch 00035: val_loss did not improve from 0.16001\n","Epoch 36/100\n"," - 49s - loss: 0.0755 - precision: 0.9718 - recall: 0.9718 - f1: 0.9718 - acc: 0.9718 - val_loss: 1.3171 - val_precision: 0.8281 - val_recall: 0.8281 - val_f1: 0.8281 - val_acc: 0.8281\n","\n","Epoch 00036: val_loss did not improve from 0.16001\n","Epoch 37/100\n"," - 49s - loss: 0.0546 - precision: 0.9814 - recall: 0.9808 - f1: 0.9811 - acc: 0.9814 - val_loss: 0.2087 - val_precision: 0.9297 - val_recall: 0.9297 - val_f1: 0.9297 - val_acc: 0.9297\n","\n","Epoch 00037: val_loss did not improve from 0.16001\n","Epoch 38/100\n"," - 49s - loss: 0.0502 - precision: 0.9813 - recall: 0.9802 - f1: 0.9807 - acc: 0.9808 - val_loss: 0.1116 - val_precision: 0.9635 - val_recall: 0.9635 - val_f1: 0.9635 - val_acc: 0.9635\n","\n","Epoch 00038: val_loss improved from 0.16001 to 0.11160, saving model to BRAIN_TUMOR_FOLD_2.h5\n","Epoch 39/100\n"," - 49s - loss: 0.0678 - precision: 0.9778 - recall: 0.9778 - f1: 0.9778 - acc: 0.9778 - val_loss: 6.3500 - val_precision: 0.4479 - val_recall: 0.4479 - val_f1: 0.4479 - val_acc: 0.4479\n","\n","Epoch 00039: val_loss did not improve from 0.11160\n","Epoch 40/100\n"," - 49s - loss: 0.0770 - precision: 0.9723 - recall: 0.9718 - f1: 0.9720 - acc: 0.9724 - val_loss: 0.3079 - val_precision: 0.9219 - val_recall: 0.9219 - val_f1: 0.9219 - val_acc: 0.9219\n","\n","Epoch 00040: val_loss did not improve from 0.11160\n","Epoch 41/100\n"," - 49s - loss: 0.0507 - precision: 0.9808 - recall: 0.9784 - f1: 0.9795 - acc: 0.9802 - val_loss: 0.3335 - val_precision: 0.8872 - val_recall: 0.8828 - val_f1: 0.8850 - val_acc: 0.8880\n","\n","Epoch 00041: val_loss did not improve from 0.11160\n","Epoch 42/100\n"," - 49s - loss: 0.0636 - precision: 0.9807 - recall: 0.9796 - f1: 0.9801 - acc: 0.9796 - val_loss: 0.2376 - val_precision: 0.9474 - val_recall: 0.9375 - val_f1: 0.9424 - val_acc: 0.9427\n","\n","Epoch 00042: val_loss did not improve from 0.11160\n","Epoch 43/100\n"," - 49s - loss: 0.0839 - precision: 0.9735 - recall: 0.9730 - f1: 0.9732 - acc: 0.9736 - val_loss: 0.7406 - val_precision: 0.7648 - val_recall: 0.7630 - val_f1: 0.7639 - val_acc: 0.7656\n","\n","Epoch 00043: val_loss did not improve from 0.11160\n","Epoch 44/100\n"," - 49s - loss: 0.0619 - precision: 0.9771 - recall: 0.9766 - f1: 0.9768 - acc: 0.9766 - val_loss: 1.4315 - val_precision: 0.7144 - val_recall: 0.7109 - val_f1: 0.7127 - val_acc: 0.7161\n","\n","Epoch 00044: val_loss did not improve from 0.11160\n","Epoch 45/100\n"," - 49s - loss: 0.0377 - precision: 0.9862 - recall: 0.9856 - f1: 0.9859 - acc: 0.9862 - val_loss: 0.1378 - val_precision: 0.9557 - val_recall: 0.9557 - val_f1: 0.9557 - val_acc: 0.9557\n","\n","Epoch 00045: val_loss did not improve from 0.11160\n","Epoch 46/100\n"," - 49s - loss: 0.0375 - precision: 0.9850 - recall: 0.9850 - f1: 0.9850 - acc: 0.9850 - val_loss: 0.4776 - val_precision: 0.8774 - val_recall: 0.8750 - val_f1: 0.8762 - val_acc: 0.8750\n","\n","Epoch 00046: val_loss did not improve from 0.11160\n","Epoch 47/100\n"," - 49s - loss: 0.0451 - precision: 0.9838 - recall: 0.9838 - f1: 0.9838 - acc: 0.9838 - val_loss: 1.9025 - val_precision: 0.6754 - val_recall: 0.6719 - val_f1: 0.6736 - val_acc: 0.6745\n","\n","Epoch 00047: val_loss did not improve from 0.11160\n","Epoch 48/100\n"," - 49s - loss: 0.0588 - precision: 0.9826 - recall: 0.9814 - f1: 0.9820 - acc: 0.9814 - val_loss: 3.8675 - val_precision: 0.5417 - val_recall: 0.5417 - val_f1: 0.5417 - val_acc: 0.5417\n","\n","Epoch 00048: val_loss did not improve from 0.11160\n","Epoch 49/100\n"," - 49s - loss: 0.0694 - precision: 0.9784 - recall: 0.9784 - f1: 0.9784 - acc: 0.9784 - val_loss: 0.7937 - val_precision: 0.7750 - val_recall: 0.7708 - val_f1: 0.7729 - val_acc: 0.7734\n","\n","Epoch 00049: val_loss did not improve from 0.11160\n","Epoch 50/100\n"," - 49s - loss: 0.0533 - precision: 0.9825 - recall: 0.9802 - f1: 0.9813 - acc: 0.9814 - val_loss: 0.4467 - val_precision: 0.8620 - val_recall: 0.8620 - val_f1: 0.8620 - val_acc: 0.8620\n","\n","Epoch 00050: val_loss did not improve from 0.11160\n","Epoch 51/100\n"," - 49s - loss: 0.0353 - precision: 0.9910 - recall: 0.9904 - f1: 0.9907 - acc: 0.9904 - val_loss: 0.6488 - val_precision: 0.8119 - val_recall: 0.8099 - val_f1: 0.8109 - val_acc: 0.8125\n","\n","Epoch 00051: val_loss did not improve from 0.11160\n","Epoch 52/100\n"," - 49s - loss: 0.0873 - precision: 0.9675 - recall: 0.9669 - f1: 0.9672 - acc: 0.9675 - val_loss: 0.6296 - val_precision: 0.8516 - val_recall: 0.8516 - val_f1: 0.8516 - val_acc: 0.8516\n","\n","Epoch 00052: val_loss did not improve from 0.11160\n","Epoch 53/100\n"," - 49s - loss: 0.0630 - precision: 0.9807 - recall: 0.9790 - f1: 0.9798 - acc: 0.9790 - val_loss: 0.5867 - val_precision: 0.8643 - val_recall: 0.8620 - val_f1: 0.8631 - val_acc: 0.8646\n","\n","Epoch 00053: val_loss did not improve from 0.11160\n","Epoch 54/100\n"," - 49s - loss: 0.0477 - precision: 0.9796 - recall: 0.9790 - f1: 0.9793 - acc: 0.9796 - val_loss: 0.1823 - val_precision: 0.9297 - val_recall: 0.9297 - val_f1: 0.9297 - val_acc: 0.9297\n","\n","Epoch 00054: val_loss did not improve from 0.11160\n","Epoch 55/100\n"," - 49s - loss: 0.0395 - precision: 0.9886 - recall: 0.9880 - f1: 0.9883 - acc: 0.9886 - val_loss: 0.2115 - val_precision: 0.9062 - val_recall: 0.9062 - val_f1: 0.9062 - val_acc: 0.9062\n","\n","Epoch 00055: val_loss did not improve from 0.11160\n","Epoch 56/100\n"," - 49s - loss: 0.0495 - precision: 0.9801 - recall: 0.9784 - f1: 0.9792 - acc: 0.9802 - val_loss: 3.6818 - val_precision: 0.7526 - val_recall: 0.7526 - val_f1: 0.7526 - val_acc: 0.7526\n","\n","Epoch 00056: val_loss did not improve from 0.11160\n","Epoch 57/100\n"," - 49s - loss: 0.0419 - precision: 0.9861 - recall: 0.9844 - f1: 0.9853 - acc: 0.9850 - val_loss: 0.8987 - val_precision: 0.7943 - val_recall: 0.7943 - val_f1: 0.7943 - val_acc: 0.7943\n","\n","Epoch 00057: val_loss did not improve from 0.11160\n","Epoch 58/100\n"," - 49s - loss: 0.0339 - precision: 0.9868 - recall: 0.9856 - f1: 0.9862 - acc: 0.9862 - val_loss: 0.3582 - val_precision: 0.9031 - val_recall: 0.8984 - val_f1: 0.9008 - val_acc: 0.8984\n","\n","Epoch 00058: val_loss did not improve from 0.11160\n","Epoch 59/100\n"," - 49s - loss: 0.0500 - precision: 0.9850 - recall: 0.9844 - f1: 0.9847 - acc: 0.9850 - val_loss: 2.3120 - val_precision: 0.7384 - val_recall: 0.7344 - val_f1: 0.7364 - val_acc: 0.7396\n","\n","Epoch 00059: val_loss did not improve from 0.11160\n","Epoch 60/100\n"," - 50s - loss: 0.0554 - precision: 0.9801 - recall: 0.9778 - f1: 0.9789 - acc: 0.9796 - val_loss: 0.1353 - val_precision: 0.9688 - val_recall: 0.9688 - val_f1: 0.9687 - val_acc: 0.9688\n","\n","Epoch 00060: val_loss did not improve from 0.11160\n","Epoch 61/100\n"," - 49s - loss: 0.0859 - precision: 0.9723 - recall: 0.9712 - f1: 0.9717 - acc: 0.9718 - val_loss: 1.9790 - val_precision: 0.6016 - val_recall: 0.6016 - val_f1: 0.6016 - val_acc: 0.6016\n","\n","Epoch 00061: val_loss did not improve from 0.11160\n","Epoch 62/100\n"," - 49s - loss: 0.0616 - precision: 0.9777 - recall: 0.9772 - f1: 0.9775 - acc: 0.9778 - val_loss: 0.7324 - val_precision: 0.7729 - val_recall: 0.7708 - val_f1: 0.7719 - val_acc: 0.7708\n","\n","Epoch 00062: val_loss did not improve from 0.11160\n","Epoch 63/100\n"," - 49s - loss: 0.0335 - precision: 0.9868 - recall: 0.9862 - f1: 0.9865 - acc: 0.9868 - val_loss: 0.1085 - val_precision: 0.9583 - val_recall: 0.9557 - val_f1: 0.9570 - val_acc: 0.9557\n","\n","Epoch 00063: val_loss improved from 0.11160 to 0.10848, saving model to BRAIN_TUMOR_FOLD_2.h5\n","Epoch 64/100\n"," - 48s - loss: 0.0480 - precision: 0.9855 - recall: 0.9844 - f1: 0.9849 - acc: 0.9844 - val_loss: 0.3257 - val_precision: 0.9085 - val_recall: 0.9036 - val_f1: 0.9061 - val_acc: 0.9036\n","\n","Epoch 00064: val_loss did not improve from 0.10848\n","Epoch 65/100\n"," - 49s - loss: 0.0720 - precision: 0.9747 - recall: 0.9724 - f1: 0.9735 - acc: 0.9730 - val_loss: 0.4230 - val_precision: 0.8408 - val_recall: 0.8385 - val_f1: 0.8396 - val_acc: 0.8411\n","\n","Epoch 00065: val_loss did not improve from 0.10848\n","Epoch 66/100\n"," - 50s - loss: 0.0451 - precision: 0.9838 - recall: 0.9832 - f1: 0.9835 - acc: 0.9832 - val_loss: 0.2796 - val_precision: 0.9139 - val_recall: 0.9115 - val_f1: 0.9127 - val_acc: 0.9115\n","\n","Epoch 00066: val_loss did not improve from 0.10848\n","Epoch 67/100\n"," - 50s - loss: 0.0378 - precision: 0.9880 - recall: 0.9868 - f1: 0.9874 - acc: 0.9868 - val_loss: 0.6393 - val_precision: 0.8516 - val_recall: 0.8516 - val_f1: 0.8516 - val_acc: 0.8516\n","\n","Epoch 00067: val_loss did not improve from 0.10848\n","Epoch 68/100\n"," - 50s - loss: 0.0399 - precision: 0.9880 - recall: 0.9880 - f1: 0.9880 - acc: 0.9880 - val_loss: 0.3182 - val_precision: 0.9115 - val_recall: 0.9115 - val_f1: 0.9115 - val_acc: 0.9115\n","\n","Epoch 00068: val_loss did not improve from 0.10848\n","Epoch 69/100\n"," - 49s - loss: 0.0785 - precision: 0.9736 - recall: 0.9736 - f1: 0.9736 - acc: 0.9736 - val_loss: 0.4886 - val_precision: 0.8721 - val_recall: 0.8698 - val_f1: 0.8709 - val_acc: 0.8698\n","\n","Epoch 00069: val_loss did not improve from 0.10848\n","Epoch 70/100\n"," - 49s - loss: 0.0636 - precision: 0.9789 - recall: 0.9766 - f1: 0.9777 - acc: 0.9772 - val_loss: 0.2871 - val_precision: 0.8906 - val_recall: 0.8906 - val_f1: 0.8906 - val_acc: 0.8906\n","\n","Epoch 00070: val_loss did not improve from 0.10848\n","Epoch 71/100\n"," - 49s - loss: 0.0528 - precision: 0.9838 - recall: 0.9820 - f1: 0.9829 - acc: 0.9832 - val_loss: 3.9504 - val_precision: 0.4453 - val_recall: 0.4453 - val_f1: 0.4453 - val_acc: 0.4453\n","\n","Epoch 00071: val_loss did not improve from 0.10848\n","Epoch 72/100\n"," - 49s - loss: 0.0450 - precision: 0.9844 - recall: 0.9838 - f1: 0.9841 - acc: 0.9838 - val_loss: 1.0113 - val_precision: 0.7573 - val_recall: 0.7552 - val_f1: 0.7562 - val_acc: 0.7578\n","\n","Epoch 00072: val_loss did not improve from 0.10848\n","Epoch 73/100\n"," - 49s - loss: 0.0414 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.5949 - val_precision: 0.8533 - val_recall: 0.8490 - val_f1: 0.8511 - val_acc: 0.8542\n","\n","Epoch 00073: val_loss did not improve from 0.10848\n","Epoch 74/100\n"," - 49s - loss: 0.0236 - precision: 0.9946 - recall: 0.9934 - f1: 0.9940 - acc: 0.9934 - val_loss: 0.1595 - val_precision: 0.9452 - val_recall: 0.9427 - val_f1: 0.9440 - val_acc: 0.9427\n","\n","Epoch 00074: val_loss did not improve from 0.10848\n","Epoch 75/100\n"," - 49s - loss: 0.0423 - precision: 0.9862 - recall: 0.9862 - f1: 0.9862 - acc: 0.9862 - val_loss: 0.2672 - val_precision: 0.9163 - val_recall: 0.9141 - val_f1: 0.9152 - val_acc: 0.9141\n","\n","Epoch 00075: val_loss did not improve from 0.10848\n","Epoch 76/100\n"," - 49s - loss: 0.0323 - precision: 0.9892 - recall: 0.9892 - f1: 0.9892 - acc: 0.9892 - val_loss: 4.4164 - val_precision: 0.5547 - val_recall: 0.5547 - val_f1: 0.5547 - val_acc: 0.5547\n","\n","Epoch 00076: val_loss did not improve from 0.10848\n","Epoch 77/100\n"," - 49s - loss: 0.0552 - precision: 0.9784 - recall: 0.9784 - f1: 0.9784 - acc: 0.9784 - val_loss: 1.3200 - val_precision: 0.7478 - val_recall: 0.7396 - val_f1: 0.7436 - val_acc: 0.7396\n","\n","Epoch 00077: val_loss did not improve from 0.10848\n","Epoch 78/100\n"," - 49s - loss: 0.0519 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 4.1543 - val_precision: 0.5078 - val_recall: 0.5078 - val_f1: 0.5078 - val_acc: 0.5078\n","\n","Epoch 00078: val_loss did not improve from 0.10848\n","Epoch 79/100\n"," - 49s - loss: 0.0404 - precision: 0.9886 - recall: 0.9880 - f1: 0.9883 - acc: 0.9880 - val_loss: 0.1110 - val_precision: 0.9634 - val_recall: 0.9609 - val_f1: 0.9622 - val_acc: 0.9635\n","\n","Epoch 00079: val_loss did not improve from 0.10848\n","Epoch 80/100\n"," - 49s - loss: 0.0321 - precision: 0.9886 - recall: 0.9886 - f1: 0.9886 - acc: 0.9886 - val_loss: 0.0946 - val_precision: 0.9557 - val_recall: 0.9557 - val_f1: 0.9557 - val_acc: 0.9557\n","\n","Epoch 00080: val_loss improved from 0.10848 to 0.09458, saving model to BRAIN_TUMOR_FOLD_2.h5\n","Epoch 81/100\n"," - 49s - loss: 0.0231 - precision: 0.9934 - recall: 0.9928 - f1: 0.9931 - acc: 0.9934 - val_loss: 0.2430 - val_precision: 0.9366 - val_recall: 0.9245 - val_f1: 0.9304 - val_acc: 0.9323\n","\n","Epoch 00081: val_loss did not improve from 0.09458\n","Epoch 82/100\n"," - 49s - loss: 0.0255 - precision: 0.9928 - recall: 0.9928 - f1: 0.9928 - acc: 0.9928 - val_loss: 4.3713 - val_precision: 0.4435 - val_recall: 0.4401 - val_f1: 0.4418 - val_acc: 0.4427\n","\n","Epoch 00082: val_loss did not improve from 0.09458\n","Epoch 83/100\n"," - 49s - loss: 0.0449 - precision: 0.9856 - recall: 0.9856 - f1: 0.9856 - acc: 0.9856 - val_loss: 7.6673 - val_precision: 0.4714 - val_recall: 0.4714 - val_f1: 0.4714 - val_acc: 0.4714\n","\n","Epoch 00083: val_loss did not improve from 0.09458\n","Epoch 84/100\n"," - 49s - loss: 0.0516 - precision: 0.9849 - recall: 0.9826 - f1: 0.9838 - acc: 0.9844 - val_loss: 0.6759 - val_precision: 0.8643 - val_recall: 0.8620 - val_f1: 0.8631 - val_acc: 0.8620\n","\n","Epoch 00084: val_loss did not improve from 0.09458\n","Epoch 85/100\n"," - 48s - loss: 0.0462 - precision: 0.9819 - recall: 0.9808 - f1: 0.9814 - acc: 0.9814 - val_loss: 0.4418 - val_precision: 0.9110 - val_recall: 0.9062 - val_f1: 0.9086 - val_acc: 0.9062\n","\n","Epoch 00085: val_loss did not improve from 0.09458\n","Epoch 86/100\n"," - 49s - loss: 0.0390 - precision: 0.9874 - recall: 0.9868 - f1: 0.9871 - acc: 0.9874 - val_loss: 0.6534 - val_precision: 0.8594 - val_recall: 0.8594 - val_f1: 0.8594 - val_acc: 0.8594\n","\n","Epoch 00086: val_loss did not improve from 0.09458\n","Epoch 87/100\n"," - 49s - loss: 0.0239 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.4850 - val_precision: 0.8620 - val_recall: 0.8620 - val_f1: 0.8620 - val_acc: 0.8620\n","\n","Epoch 00087: val_loss did not improve from 0.09458\n","Epoch 88/100\n"," - 49s - loss: 0.0306 - precision: 0.9885 - recall: 0.9874 - f1: 0.9880 - acc: 0.9880 - val_loss: 0.5014 - val_precision: 0.8483 - val_recall: 0.8438 - val_f1: 0.8460 - val_acc: 0.8464\n","\n","Epoch 00088: val_loss did not improve from 0.09458\n","Epoch 89/100\n"," - 49s - loss: 0.0320 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 3.6681 - val_precision: 0.5443 - val_recall: 0.5443 - val_f1: 0.5443 - val_acc: 0.5443\n","\n","Epoch 00089: val_loss did not improve from 0.09458\n","Epoch 90/100\n"," - 49s - loss: 0.0746 - precision: 0.9735 - recall: 0.9718 - f1: 0.9726 - acc: 0.9724 - val_loss: 8.8216 - val_precision: 0.3568 - val_recall: 0.3568 - val_f1: 0.3568 - val_acc: 0.3568\n","\n","Epoch 00090: val_loss did not improve from 0.09458\n","Epoch 91/100\n"," - 49s - loss: 0.0391 - precision: 0.9880 - recall: 0.9874 - f1: 0.9877 - acc: 0.9874 - val_loss: 1.2473 - val_precision: 0.7292 - val_recall: 0.7292 - val_f1: 0.7292 - val_acc: 0.7292\n","\n","Epoch 00091: val_loss did not improve from 0.09458\n","Epoch 92/100\n"," - 49s - loss: 0.0347 - precision: 0.9892 - recall: 0.9880 - f1: 0.9886 - acc: 0.9880 - val_loss: 0.4875 - val_precision: 0.8485 - val_recall: 0.8464 - val_f1: 0.8474 - val_acc: 0.8490\n","\n","Epoch 00092: val_loss did not improve from 0.09458\n","Epoch 93/100\n"," - 49s - loss: 0.0341 - precision: 0.9880 - recall: 0.9868 - f1: 0.9874 - acc: 0.9880 - val_loss: 0.1553 - val_precision: 0.9557 - val_recall: 0.9557 - val_f1: 0.9557 - val_acc: 0.9557\n","\n","Epoch 00093: val_loss did not improve from 0.09458\n","Epoch 94/100\n"," - 49s - loss: 0.0251 - precision: 0.9946 - recall: 0.9934 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.3296 - val_precision: 0.8928 - val_recall: 0.8880 - val_f1: 0.8904 - val_acc: 0.8906\n","\n","Epoch 00094: val_loss did not improve from 0.09458\n","Epoch 95/100\n"," - 49s - loss: 0.0351 - precision: 0.9874 - recall: 0.9868 - f1: 0.9871 - acc: 0.9874 - val_loss: 0.4608 - val_precision: 0.8876 - val_recall: 0.8828 - val_f1: 0.8852 - val_acc: 0.8880\n","\n","Epoch 00095: val_loss did not improve from 0.09458\n","Epoch 96/100\n"," - 49s - loss: 0.0600 - precision: 0.9808 - recall: 0.9796 - f1: 0.9802 - acc: 0.9796 - val_loss: 4.5663 - val_precision: 0.4766 - val_recall: 0.4740 - val_f1: 0.4753 - val_acc: 0.4740\n","\n","Epoch 00096: val_loss did not improve from 0.09458\n","Epoch 97/100\n"," - 49s - loss: 0.0445 - precision: 0.9862 - recall: 0.9862 - f1: 0.9862 - acc: 0.9862 - val_loss: 0.3342 - val_precision: 0.9062 - val_recall: 0.9062 - val_f1: 0.9062 - val_acc: 0.9062\n","\n","Epoch 00097: val_loss did not improve from 0.09458\n","Epoch 98/100\n"," - 49s - loss: 0.0427 - precision: 0.9826 - recall: 0.9820 - f1: 0.9823 - acc: 0.9826 - val_loss: 0.2560 - val_precision: 0.9080 - val_recall: 0.9036 - val_f1: 0.9058 - val_acc: 0.9036\n","\n","Epoch 00098: val_loss did not improve from 0.09458\n","Epoch 99/100\n"," - 49s - loss: 0.0323 - precision: 0.9904 - recall: 0.9892 - f1: 0.9898 - acc: 0.9892 - val_loss: 0.1009 - val_precision: 0.9609 - val_recall: 0.9609 - val_f1: 0.9609 - val_acc: 0.9609\n","\n","Epoch 00099: val_loss did not improve from 0.09458\n","Epoch 100/100\n"," - 49s - loss: 0.0233 - precision: 0.9940 - recall: 0.9928 - f1: 0.9934 - acc: 0.9934 - val_loss: 1.1269 - val_precision: 0.7318 - val_recall: 0.7318 - val_f1: 0.7318 - val_acc: 0.7318\n","\n","Epoch 00100: val_loss did not improve from 0.09458\n","Epoch 1/100\n"," - 51s - loss: 0.1164 - precision: 0.9615 - recall: 0.9603 - f1: 0.9609 - acc: 0.9603 - val_loss: 2.7491 - val_precision: 0.5990 - val_recall: 0.5990 - val_f1: 0.5990 - val_acc: 0.5990\n","\n","Epoch 00001: val_loss improved from inf to 2.74913, saving model to BRAIN_TUMOR_FOLD_3.h5\n","Epoch 2/100\n"," - 47s - loss: 0.0639 - precision: 0.9771 - recall: 0.9760 - f1: 0.9765 - acc: 0.9772 - val_loss: 1.3334 - val_precision: 0.7588 - val_recall: 0.7552 - val_f1: 0.7570 - val_acc: 0.7578\n","\n","Epoch 00002: val_loss improved from 2.74913 to 1.33344, saving model to BRAIN_TUMOR_FOLD_3.h5\n","Epoch 3/100\n"," - 48s - loss: 0.0586 - precision: 0.9807 - recall: 0.9778 - f1: 0.9792 - acc: 0.9784 - val_loss: 1.8767 - val_precision: 0.5911 - val_recall: 0.5911 - val_f1: 0.5911 - val_acc: 0.5911\n","\n","Epoch 00003: val_loss did not improve from 1.33344\n","Epoch 4/100\n"," - 49s - loss: 0.0843 - precision: 0.9712 - recall: 0.9694 - f1: 0.9702 - acc: 0.9700 - val_loss: 2.8072 - val_precision: 0.5469 - val_recall: 0.5469 - val_f1: 0.5469 - val_acc: 0.5469\n","\n","Epoch 00004: val_loss did not improve from 1.33344\n","Epoch 5/100\n"," - 49s - loss: 0.1027 - precision: 0.9649 - recall: 0.9615 - f1: 0.9632 - acc: 0.9627 - val_loss: 11.1931 - val_precision: 0.2891 - val_recall: 0.2891 - val_f1: 0.2891 - val_acc: 0.2891\n","\n","Epoch 00005: val_loss did not improve from 1.33344\n","Epoch 6/100\n"," - 50s - loss: 0.0945 - precision: 0.9734 - recall: 0.9712 - f1: 0.9723 - acc: 0.9736 - val_loss: 0.7656 - val_precision: 0.7865 - val_recall: 0.7865 - val_f1: 0.7865 - val_acc: 0.7865\n","\n","Epoch 00006: val_loss improved from 1.33344 to 0.76558, saving model to BRAIN_TUMOR_FOLD_3.h5\n","Epoch 7/100\n"," - 49s - loss: 0.0943 - precision: 0.9633 - recall: 0.9615 - f1: 0.9624 - acc: 0.9621 - val_loss: 0.2105 - val_precision: 0.9259 - val_recall: 0.9115 - val_f1: 0.9186 - val_acc: 0.9115\n","\n","Epoch 00007: val_loss improved from 0.76558 to 0.21052, saving model to BRAIN_TUMOR_FOLD_3.h5\n","Epoch 8/100\n"," - 48s - loss: 0.0603 - precision: 0.9783 - recall: 0.9778 - f1: 0.9781 - acc: 0.9778 - val_loss: 0.4205 - val_precision: 0.8620 - val_recall: 0.8620 - val_f1: 0.8620 - val_acc: 0.8620\n","\n","Epoch 00008: val_loss did not improve from 0.21052\n","Epoch 9/100\n"," - 49s - loss: 0.0454 - precision: 0.9838 - recall: 0.9838 - f1: 0.9838 - acc: 0.9838 - val_loss: 0.5491 - val_precision: 0.8638 - val_recall: 0.8594 - val_f1: 0.8616 - val_acc: 0.8620\n","\n","Epoch 00009: val_loss did not improve from 0.21052\n","Epoch 10/100\n"," - 49s - loss: 0.0535 - precision: 0.9820 - recall: 0.9814 - f1: 0.9817 - acc: 0.9820 - val_loss: 5.1904 - val_precision: 0.4818 - val_recall: 0.4818 - val_f1: 0.4818 - val_acc: 0.4818\n","\n","Epoch 00010: val_loss did not improve from 0.21052\n","Epoch 11/100\n"," - 49s - loss: 0.0796 - precision: 0.9723 - recall: 0.9706 - f1: 0.9714 - acc: 0.9712 - val_loss: 0.6718 - val_precision: 0.8259 - val_recall: 0.8125 - val_f1: 0.8191 - val_acc: 0.8177\n","\n","Epoch 00011: val_loss did not improve from 0.21052\n","Epoch 12/100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3I83S5TU4jZX"},"source":["def get_test_data():\n","    gen = DATASET(SHAPE, BATCH_SIZE, range(1), BASE_DIR, SEED, TRAIN_TEST_RATIO, augment=False).split_train_test(\"test\")\n","                       \n","    x = np.empty((len(gen[0]),)+SHAPE, dtype=np.float32)\n","    y = np.empty((len(gen[1]), 3), dtype=np.float32)\n","    \n","    for ix, path in tqdm(enumerate(gen[0])):\n","        img = np.array(Image.open(gen[0][ix]))\n","        img = resize(img, SHAPE)\n","\n","        label = gen[1][ix]\n","\n","        x[ix] = img\n","        y[ix] = label\n","        \n","    return x, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EDnun27N4jZa","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8e904195-4a5f-46b7-b969-3d0246cf669b"},"source":["x, y = get_test_data()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["912it [05:22,  2.72it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"uCjjF4vm4jZd"},"source":["# Threshold predictions with THRESH_VAL\n","def threshold_arr(array):\n","    # Get all value from array\n","    # Compare calue with THRESH_VAL \n","    # IF value >= THRESH_VAL. round to 1\n","    # ELSE. round to 0\n","    new_arr = []\n","    for ix, val in enumerate(array):\n","        loc = np.array(val).argmax(axis=0)\n","        k = list(np.zeros((len(val)), dtype=np.float32))\n","        k[loc]=1\n","        new_arr.append(k)\n","        \n","    return np.array(new_arr, dtype=np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIOdFm1Y4jZg","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"6ebb7ef4-60c8-4ffe-f99e-f021c4ebcc65"},"source":["\n","models = []\n","for i in range(5):\n","    model = load_model(\"BRAIN_TUMOR_FOLD_{}.h5\".format(i), custom_objects={'f1': f1, 'precision': precision, 'recall': recall})\n","    print(model.evaluate(x, y, verbose=0))\n","    models.append(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.16067420998424814, 0.9538766271189639, 0.9517543859649122, 0.9527986206506428, 0.9539473684210527]\n","[0.1294135550739603, 0.9594298245614035, 0.9594298245614035, 0.9594297816878871, 0.9594298245614035]\n","[0.13585177619467703, 0.9539473684210527, 0.9539473684210527, 0.9539473213647541, 0.9539473684210527]\n","[0.12272553212875337, 0.9649122807017544, 0.9638157894736842, 0.9643552899360657, 0.9638157894736842]\n","[0.139483545083765, 0.9560342378783644, 0.9539473684210527, 0.9549741943677267, 0.9539473684210527]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q4K00nrl4jZj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XVq6AwMb4jZn"},"source":["def plot_confusion_matrix(cm,\n","                          target_names,\n","                          title='Confusion matrix',\n","                          cmap=None,\n","                          normalize=True):\n","    \"\"\"\n","    given a sklearn confusion matrix (cm), make a nice plot\n","\n","    Arguments\n","    ---------\n","    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n","\n","    target_names: given classification classes such as [0, 1, 2]\n","                  the class names, for example: ['high', 'medium', 'low']\n","\n","    title:        the text to display at the top of the matrix\n","\n","    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n","                  see http://matplotlib.org/examples/color/colormaps_reference.html\n","                  plt.get_cmap('jet') or plt.cm.Blues\n","\n","    normalize:    If False, plot the raw numbers\n","                  If True, plot the proportions\n","\n","    Usage\n","    -----\n","    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n","                                                              # sklearn.metrics.confusion_matrix\n","                          normalize    = True,                # show proportions\n","                          target_names = y_labels_vals,       # list of names of the classes\n","                          title        = best_estimator_name) # title of graph\n","\n","    Citiation\n","    ---------\n","    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n","\n","    \"\"\"\n","    import matplotlib.pyplot as plt\n","    import numpy as np\n","    import itertools\n","\n","    accuracy = np.trace(cm) / float(np.sum(cm))\n","    misclass = 1 - accuracy\n","\n","    if cmap is None:\n","        cmap = plt.get_cmap('Blues')\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","\n","    if target_names is not None:\n","        tick_marks = np.arange(len(target_names))\n","        plt.xticks(tick_marks, target_names, rotation=45)\n","        plt.yticks(tick_marks, target_names)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","\n","    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        if normalize:\n","            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","        else:\n","            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n","    plt.savefig(\"confusion matrix_best.jpg\", dpi=150)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"57JHUoqj4jZr","colab":{"base_uri":"https://localhost:8080/","height":739},"outputId":"c2155581-8587-4742-d5cc-0df7a4c7bf7c"},"source":["y_preds = threshold_arr(models[4].predict(x, verbose=0))\n","\n","results = precision_recall_fscore_support(y, y_preds ,average='macro')\n","acc = accuracy_score(y, y_preds)\n","\n","print(\"Accuracy: {}, F1_Score: {}, Precision: {}, Recall: {}\".format(acc, results[2], results[0], results[1]))\n","print(\"\\n\")\n","print(classification_report(y, y_preds))\n","print(\"\\n\")\n","cnf_matrix = confusion_matrix(y.argmax(axis=1), y_preds.argmax(axis=1))\n","\n","plot_confusion_matrix(cm           = cnf_matrix, \n","                      normalize    = False,\n","                      target_names = ['Meningioma', 'Glioma','Pituitary'],\n","                      title        = \"Confusion Matrix\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.9539473684210527, F1_Score: 0.9490549864229975, Precision: 0.9568781429712478, Recall: 0.9429232369373214\n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.87      0.91       222\n","           1       0.94      0.99      0.96       426\n","           2       0.97      0.97      0.97       264\n","\n","   micro avg       0.95      0.95      0.95       912\n","   macro avg       0.96      0.94      0.95       912\n","weighted avg       0.95      0.95      0.95       912\n"," samples avg       0.95      0.95      0.95       912\n","\n","\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAfYAAAHCCAYAAAAdAOsHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 576x432 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"d2v7WuVR4jZw","colab":{"base_uri":"https://localhost:8080/","height":394},"outputId":"7ba8cfc4-2ae8-45aa-d0a7-4d44f1bcbcd3"},"source":["from sklearn.metrics import auc, roc_curve\n","\n","y_pred = models[4].predict(x, verbose=0)\n","y_proba = []\n","for ix, i in enumerate(y_pred.argmax(axis=1)):\n","    if i==0:\n","        y_proba.append(1-y_pred[ix][i])\n","    else:\n","        y_proba.append(y_pred[ix][i])\n","        \n","fpr, tpr, thresholds = roc_curve(y.argmax(axis=1), y_proba)\n","roc_auc = auc(fpr, tpr)\n","\n","plt.figure()\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.plot(fpr, tpr, label='Brain Tumor Classification (AUC = {})'.format(round(roc_auc,3)))\n","plt.legend(loc=\"lower right\")\n","plt.savefig(\"ROC_best.jpg\", dpi=150)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-8e21f008e704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my_proba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \"\"\"\n\u001b[1;32m    770\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 771\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    534\u001b[0m     if not (y_type == \"binary\" or\n\u001b[1;32m    535\u001b[0m             (y_type == \"multiclass\" and pos_label is not None)):\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"]}]},{"cell_type":"code","metadata":{"id":"xWpEUz0e4jZz"},"source":["import os\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from tensorboard.backend.event_processing import event_accumulator  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D6mn8R-hrQSY"},"source":["if not os.path.exists(\"graphs\"):\n","    os.makedirs(\"graphs\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5RxwJGzrUIo"},"source":["FOLDS = [\"log_0\", \"log_1\", \"log_2\", \"log_3\", \"log_4\"]\n","\n","TRAIN_METRIC_N_LOSS = [\"acc\", \"f1\", \"precision\", \"recall\", \"loss\"]\n","TEST_METRIC_N_LOSS  = [\"val_acc\", \"val_f1\", \"val_precision\", \"val_recall\", \"val_loss\"]\n","\n","skip_N = 27"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GFApC6VDrY2W"},"source":["tbs = []\n","for fold in FOLDS:\n","    tbs.append(os.listdir(fold)[0])\n","\n","for k, nth_fold in enumerate(FOLDS):\n","    train_logs = []\n","    test_logs  = []\n","    \n","    ea = event_accumulator.EventAccumulator(nth_fold+\"/\"+tbs[k],\n","                                            size_guidance={ # see below regarding this argument\n","                                                event_accumulator.COMPRESSED_HISTOGRAMS: 800,\n","                                                event_accumulator.IMAGES: 4,\n","                                                event_accumulator.AUDIO: 4,\n","                                                event_accumulator.SCALARS: 0,\n","                                                event_accumulator.HISTOGRAMS: 1})\n","\n","    ea.Reload()\n","    for i in range(len(TRAIN_METRIC_N_LOSS)):\n","        train_logs.append(pd.DataFrame(ea.Scalars(TRAIN_METRIC_N_LOSS[i])))\n","        test_logs.append(pd.DataFrame(ea.Scalars(TEST_METRIC_N_LOSS[i])))\n","    \n","    globals()[nth_fold] = [train_logs, test_logs]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPqDyYe7rZmV"},"source":["for m in FOLDS:\n","    n_train_logs = []\n","    n_test_logs  = []\n","    \n","    for (train, test) in zip(globals()[m][0], globals()[m][1]):\n","        n_train = []\n","        n_test  = []\n","\n","        for ix, i in enumerate(train['value'].tolist()):\n","            if ix%skip_N==0:\n","                n_train.append(i)\n","\n","        for ix, i in enumerate(test['value'].tolist()):\n","            if ix%1==0:\n","                n_test.append(i)\n","\n","        n_train_logs.append(n_train)\n","        n_test_logs.append(n_test)\n","    \n","    globals()[\"n_\"+m] = [n_train_logs, n_test_logs]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKJSPT9VrccU","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"error","timestamp":1631078070801,"user_tz":-180,"elapsed":5,"user":{"displayName":"mesut toğaçar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3up8QS6_DqT3WYhCPruF_FXCqGxayyZYqWN5v=s64","userId":"08628900079395750792"}},"outputId":"7fa9ab90-efdf-40f1-c2a7-01ffb50ff823"},"source":["for ix, fold in enumerate(FOLDS):\n","    plt.plot(globals()[\"n_\"+fold][0][0])\n","    plt.plot(globals()[\"n_\"+fold][1][0])\n","    plt.title('Model Accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'val'], loc='lower right')\n","    plt.savefig(\"graphs/{}. FOLD - MODEL ACCURACY.jpg\".format(ix+1), dpi=150)\n","    plt.show()"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2b1f828dbe7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOLDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'FOLDS' is not defined"]}]},{"cell_type":"code","metadata":{"id":"SN5Wf5qVrilD","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"error","timestamp":1631078077221,"user_tz":-180,"elapsed":823,"user":{"displayName":"mesut toğaçar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3up8QS6_DqT3WYhCPruF_FXCqGxayyZYqWN5v=s64","userId":"08628900079395750792"}},"outputId":"dcead734-3f1c-4a8b-d648-32a59daabc0e"},"source":["for ix, fold in enumerate(FOLDS):\n","    plt.plot(globals()[\"n_\"+fold][0][4])\n","    plt.plot(globals()[\"n_\"+fold][1][4])\n","    plt.title('Model Loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.savefig(\"graphs/{}. FOLD - MODEL LOSS.jpg\".format(ix+1), dpi=150)\n","    plt.show()"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-7b4b07267699>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOLDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'FOLDS' is not defined"]}]},{"cell_type":"code","metadata":{"id":"E7e4LEBFBUCR"},"source":["from google.colab import files\n","files.download(\"/content/log_0/events.out.tfevents.1580467798.67a5368b2173\")"],"execution_count":null,"outputs":[]}]}